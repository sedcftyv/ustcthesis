% !TeX root = ../main.tex

\chapter{基于混合专家模型的编码配置选择器}

\section{引言}

上一章提出的可微纹理块压缩模型中，为了在多种编码配置中选择最佳配置，
采用了与标准DXT格式编解码器类似的方法，即在可微编解码过程中使用所有编码配置，
并根据每种编码配置的压缩误差选择最佳编码配置。这种方法简单并且易于实现，
但会造成显著的计算开销。为了优化这种编码配置的计算效率，本章提出了一种基于混合专家模型的编码配置选择器，
这个模块将替换上一章中的配置选择器，在较少压缩质量损失的前提下，提高可微编解码器的计算效率。

\section{问题描述}

对于DXT格式中的多模式格式（例如BC6与BC7），对于纹理中的每4x4纹理块，编码时都会选择
其中一种编码配置进行编码（例如某种特定的分区操作或将RGB通道之一与A通道交换的旋转操作等）。
在标准DXT格式编解码器中，为了确定最佳的编码配置，通常会使用多种编码配置进行编解码，并选择使得压缩误差最低的
编码配置作为最优配置。对于面向压缩任务的标准编解码器，这个过程对于某个纹理块只进行一次，因此带来的计算开销可以被接受。
但对于本文提出的面向优化任务的可微编解码器，这个过程会在优化过程中每次迭代进行一次，因此带来的计算开销比较显著。
例如BC6具有单分区与32种双分区共33种不同配置，BC7具有4种旋转通道的单分区以及64种双分区共68种不同配置，
计算所有编码配置意味着对于BC6需要将计算量增加至33倍，对于BC7是68倍。
为解决这一问题，本章提出了一种基于混合专家模型的编码配置选择器。

\section{模型框架}

最初的混合专家模型\inlinecite{jacobs1991adaptive}中，每个专家都代表一个前馈神经网络，
还有一个门控网络用于选择使用哪个专家的结果作为输出。专家网络和门控网络都接收相同的输入，
门控网络将输入乘上一个可训练的权重矩阵后，使用softmax层获得每个专家的概率分布，
最后根据这个概率分布在所有专家中随机采样其中一个并作为输出。
最近的DeepSeekMoE\cite{dai2024deepseekmoe}将一部分专家隔离为共享专家，其他专家被称为路由专家。
共享专家的输出总是有效。路由专家通过门控网络控制，门控网络的输出被称为亲和度(affinity)，
亲和度最高的前k个路由专家的输出有效，然后通过亲和度对有效的路由专家的输出进行加权混合。
最终的输出为将共享专家与加权后的有效路由专家两部分输出之和。

纹理编解码过程中一种编码配置可以视作一个专家，DeepSeekMoE\cite{dai2024deepseekmoe}混合多个专家的结果，
但纹理编解码对于每个纹理块仅用一种编码配置，因此可以通过混合专家模型，
选出优先级较高的编码配置（专家），再在这些编码配置中选出最佳的一个，从而
避免上一章中计算所有编码配置带来的较大计算开销。

DeepSeekMoE\cite{dai2024deepseekmoe}中的
门控网络输出路由专家对输入的亲和度，并根据亲和度选择路由专家，
而纹理编解码过程只需要选择压缩误差最小的编码配置，因此可以使用压缩误差的倒数作为对编码配置的亲和度，同时无需使用门控网络，即：
\begin{equation}
s_i=\frac{1}{L_i}
\end{equation}
其中$s_i$表示编码配置$i$的亲和度，
$L_i$表示使用编码配置$i$产生的压缩误差。

由于优化过程迭代进行，每次优化时数据发生的变化较小，可以使用之前迭代中计算出的亲和度
近似当前迭代的亲和度。即优化前初始化一个亲和度表$\{s_i|i\in\mathcal{I}\}$，
记录每种编码配置的亲和度，并在每次迭代中根据当前迭代选中的编码配置计算出的亲和度更新亲和度表。

假设某次迭代选中的编码配置集合为$\{i|i\in\mathcal{I}^*\}$，更新亲和度表时：
\begin{equation}
    s_i\leftarrow
    \begin{cases} 
    \frac{1}{\|B-B'_i\|_F^2}, & \text{if } i\in\mathcal{I}^*,\\
    s_i, & \text{otherwise}.
    \end{cases}
\end{equation}
其中$B$表示可微编解码前的4x4纹理块，$B'_i$表示$B$使用编码配置$i$可微编解码后的4x4纹理块。

亲和度表存储了每4x4纹理块中每个编码配置对应的亲和度。
基于亲和度表可以使用Topk方法选出优先级较高的编码配置。
% 亲和度表存储每4x4块中每个编码配置对应的亲和度，编码时根据亲和度表选择前$k$个最佳配置，
% 首次迭代前初始化整个压缩误差表，每次迭代更新亲和度表。由于每个纹理块仅需要一个编码配置，
% 对于选出的前$k$个配置，还需要分别进行编解码，最后根据编码误差选择其中最好的一个编码配置。
具体来说，首先根据亲和度表从所有编码配置集 $\mathcal{I}$ 中选出亲和度最大的前$k$个编码配置$\mathcal{I}^*$ ，即
\begin{equation}
\label{eq41}
\mathcal{I}^*=\left\{i|s_i\in\text{Topk}(\{s_i|j\in\mathcal{I}\},k)\right\}
\end{equation}
其中 $\text{Topk}(\mathcal{S},n)$ 表示从集合 $\mathcal{S}$ 中选出最大的 $n$ 个元素组成的集合，
$s_i$表示亲和度表中编码配置$i$的亲和度。

该方法使用的亲和度表仅根据历史迭代数据计算，这种近似
导致所使用的亲和度表与当前迭代的真实值有一定差异。

与\inlinecite{shazeer2017outrageously}中的Noisy Top-K门控类似，可以
在对亲和度表进行Topk计算时，通过增加噪声的方式，实现编码配置的随机采样，
从而在优化初期尝试更多的编码配置，降低亲和度表近似产生的误差。
因此向\eqref{eq41}中添加了Gumbel噪声\cite{jang2016categorical}实现将亲和度作为概率采样编码配置：
\begin{equation}
\mathcal{I}^*=\left\{i|s_i+\tau*g_j\in\text{Topk}(\{s_i+\tau*g_j|j\in\mathcal{I}\},k)\right\}
\end{equation}
其中$g_j$表示从$Gumbel(0,1)$分布中获得的噪声样本，$\tau$表示温度系数，$\tau$较大时
噪声的权重较大，编码配置的选择过程接近随机采样，$\tau$越小，编码配置选择过程的随机性也越小。

这种方法在优化初期引入的噪声可能会造成优化过程不稳定的问题，
因此与DeepSeekMoE\cite{dai2024deepseekmoe}将混合专家模型中的专家划分为共享专家与路由专家两部分类似，
将选择编码配置的过程划分为选择稳定配置$\mathcal{I}_s$与选择随机配置$\mathcal{I}_r$两部分：
\begin{equation}
\label{eq2}
    \mathcal{I}_s=\left\{i|s_i\in\text{Topk}(\{s_i|j\in\mathcal{I}\},N_s)\right\}
\end{equation}
\begin{equation}
\label{eq3}
\mathcal{I}_r=\left\{i|s_i+\tau*g_j\in\text{Topk}(\{s_i+\tau*g_j|j\in\mathcal{I}-\mathcal{I}_s\},N_r)\right\}
\end{equation}
首先利用\eqref{eq2}计算出$N_s$个稳定配置$\mathcal{I}_s$，然后利用\eqref{eq3}在剩余配置中计算出$N_r$个随机配置$\mathcal{I}_r$。
最终选中的编码配置集合$\mathcal{I}^*$为：
\begin{equation}
\mathcal{I}^*=\mathcal{I}_{s}\cup\mathcal{I}_r
\end{equation}
接下来使用上一章提出的编码配置选择器选择出最佳配置。

设输入纹理块 $\mathbf{B}\in\mathbb{R}^{b\times n}$，其中 $b$ 是一个纹理块中的纹素个数。
基于混合专家模型选中的编码配置集合为 $\mathcal{I}^*$，$\mathbf{B}$使用配置 $i\in\mathcal{I}^*$ 
经过可微编解码后得到的纹理块为 $\mathbf{B}'_i\in\mathbb{R}^{b\times n}$，
对应的压缩误差为 $L_i$。
\begin{equation}
L_i=\|\mathbf{B}-\mathbf{B}'_i\|_F^2
\end{equation}
\begin{equation}
\mathbf{B}'_{i^*}=\sum_{i\in\mathcal{I}} w_i\mathbf{B}'_i
\end{equation}
\begin{equation}
    w_i =\left\{\begin{matrix}
        1,& i=\mathop{\arg\min}\limits_{i} L_i
        \\0,& i\ne\mathop{\arg\min}\limits_{i} L_i
        \end{matrix}\right.
\end{equation}
其中 $w_i$ 是配置 $i$ 的组合系数。由于最终只选择使得$L_i$最小的配置，
因此实际上 $w_i$ 是 one-hot 的。
设所有编码配置集合$\mathcal{I}$中的配置数为$N$，
这个方法的计算量是原来的$\frac{N_s+N_r}{N}$，当$N_s+N_r=N$时等价于上一章中的方法。

该方法的整体框架如图\ref{fig:Mop}所示，其优点包括：
\begin{itemize}
\item 稳定性：稳定编码配置保证总是会计算亲和度较高的编码配置，使得训练更稳定。
\item 探索性：随机编码配置保证总是会探索剩余的编码配置，因为即使有些配置在某次优化迭代中亲和度不高，
但随着训练的进行，这些配置的亲和度可能会变高。
\item 效率：通过结合稳定编码配置与随机编码配置的选择策略，
该模型在确保训练稳定性和探索性的同时，使计算效率提高了一个数量级。
\end{itemize}
\begin{figure}[htbp]
    \centering
    \includegraphics[width=1\textwidth]{figures/Mop_v2.pdf}
    \caption{基于混合专家模型的编码配置选择器}
    \label{fig:Mop}
\end{figure}


\section{实验设计与分析}


