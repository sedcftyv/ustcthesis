% !TeX root = ../main.tex

\chapter{基于混合专家模型的分区配置选择器}

\section{引言}

上一章提出的可微块压缩模型中，为了在多种编码配置中选择最佳配置，
采用了与标准DXT格式编解码器类似的方法，即在可微编解码过程中尝试所有编码配置，
并根据每种编码配置的压缩误差选择最佳编码配置。这种方法简单并且易于实现，
但会造成显著的计算开销。
例如上一章中的DBC6在每次迭代过程进行可微编解码时，需要计算32种双分区+1种单分区共33种编码配置，
DBC7需要计算64种双分区+1种单分区+4种旋转\times3种不同量化方案共77中编码配置，
相较于仅计算一种编码配置的固定编码配置的DBCs模型，显著增加了运行时间。
分区配置在BC6/BC7中的所有编码配置中占比最高（BC6格式分区配置占配置总数的33/34，BC7格式分区配置占配置总数的
64/77），因此优化大量分区配置的计算即可显著提高编码配置选择过程的计算效率。
为了优化大量分区配置造成的计算开销，
本章提出了一种基于混合专家模型的分区配置选择器，
这个模块的作用是从大量分区配置中启发式地选择出
优先级较高的分区配置，通过只使用优先级较高的
分区配置进行编解码，
在几乎不影响编码质量的前提下，
提高可微块压缩模型(DBC)的计算效率。

\section{问题描述与分析}

对于DXT格式中的多模式格式（例如BC6与BC7），纹理中的每4\times4块编码时都会选择
其中一种编码配置进行编码（例如不同的颜色端点与权重量化位数、
某种分区方案或将RGB通道之一与A通道交换的旋转等）。
在标准DXT格式编解码器中，为了确定最佳的编码配置，通常会使用多种编码配置进行编解码，并选择使得压缩误差最低的
编码配置作为最优配置。对于面向压缩任务的标准编解码器，
这个过程对于每个纹理块只进行一次，因此带来的计算开销可以被接受。
但对于面向优化任务的可微编解码器，这个过程会在优化过程中每次迭代进行一次，
因此带来的计算开销非常显著。
而分区配置在BC6/BC7中的所有编码配置中占绝大部分（BC6格式分区配置占配置总数的33/34，BC7格式分区配置占配置总数的
64/77），因此优化分区配置的选择可以减少编码配置选择过程中主要的计算开销。

为解决这一问题，首先需要分析上一章中基于穷举法的编码配置选择过程。
设输入纹理块 $\mathbf{B}\in\mathbb{R}^{b\times n}$，其中 $b$ 是一个纹理块中的纹素个数。
所有配置的集合为 $\mathcal{I}$，针对配置 $i\in\mathcal{I}$ 经过可微编解码后得到的压缩纹理块为 $\mathbf{B}'_i\in\mathbb{R}^{b\times n}$。
定义编码配置$i$对应的压缩误差 $L_i$ 为
\begin{equation}
L_i=\|\mathbf{B}-\mathbf{B}'_i\|_F^2
\end{equation}
对每4\times4纹理块$\mathbf{B}$，配置选择器选择使得压缩误差$L_i$最小的编码配置$i^*$：
\begin{equation}
    \label{4:argmin}
    i^*=\mathop{\arg\min}\limits_{i} L_i
\end{equation}
为了确定每种编码配置$i$对应的编码误差$L_i$，
上一章中使用了穷举的方法，对每4\times4纹理块都会使用所有编码配置进行编码，
从而计算每4\times4纹理块所有编码配置对应的编码误差。
由于优化过程需要进行大量迭代，因此对编码配置的穷举造成了显著的计算开销。

根据公式\ref{4:argmin}可知，即使计算出了每种编码配置$i$对应的编码误差$L_i$，
最终有效的编码配置依然只有其中一个，因此大部分的计算对最终结果都没有
任何贡献。如果在不计算每种编码配置的编码误差的情况下，依旧能选择出
最佳的编码配置，就可以大量节省计算开销。

一种启发式的想法是，迭代过程开始前初始化一个编码误差表，其中存储
每4\times4块中每种编码配置对应的编码误差。
每次迭代中根据编码误差表选择一些编码误差较小的编码配置进行计算，
同时迭代过程始终保持更新编码误差表。
相当于使用历史迭代的编码误差估计当前迭代中的编码误差。

这种方法每次迭代只需计算少量编码配置，因此可以显著节省计算开销。
但与之而来的问题是该方法只更新每次迭代中被选中（激活）的少量编码配置，
可能导致优化过程中每个块的编码配置趋于固定，导致DBC模型退化为
上一章中固定编码配置的DBCs模型。

可以使用强化学习中智能体的探索策略，每次迭代中
除了选择编码误差较小的编码配置外，对于剩余的编码配置
按照对应的编码误差进行随机采样，使编码误差暂时较大的编码配置
也有被选中的可能，防止频繁地只选中少量编码配置，导致方法退化。

因此本章提出了一种基于混合专家模型的分区配置选择器，每次迭代过程中
根据编码误差表将分区配置分为稳定分区配置与随机分区配置，
从而兼顾对编码配置表的探索与利用，在保持较高编码质量的前提下，
减少分区配置选择造成的大量计算开销。

\section{模型框架}

最初的混合专家模型\cite{jacobs1991adaptive}中，每个专家都代表一个前馈神经网络，
还有一个门控网络用于选择使用哪个专家的结果作为输出。专家网络和门控网络都接收相同的输入，
门控网络将输入乘上一个可训练的权重矩阵后，使用softmax层获得每个专家的概率分布，
最后根据这个概率分布在所有专家中随机采样其中一个并作为输出。
最近的DeepSeekMoE\cite{dai2024deepseekmoe}将一部分专家隔离为共享专家，其他专家被称为路由专家。
共享专家的输出总是有效。路由专家通过门控网络控制，门控网络的输出被称为亲和度(affinity)，
亲和度最高的前k个路由专家的输出有效，然后通过亲和度对有效的路由专家的输出进行加权混合。
最终的输出为将共享专家与加权后的有效路由专家两部分输出之和。

纹理编解码过程中一种分区编码配置可以视作一个专家。
对于分区编码配置的选择基于编码误差，因此可以使用分区配置的编码误差表替换门控网络
用于进行专家选择。为了减少大量分区配置造成的计算开销，
每次迭代过程中从分区配置的编码误差表中
只选择一部分分区配置作为激活配置，
与其余非分区配置一起进行可微编解码并计算编码误差，
同时更新分区配置的编码误差表，
最后通过配置选择器根据编码误差输出最终的可微编解码结果。

由于每次计算过程中实际只计算并更新了部分分区配置的编码误差，
为了防止退化为固定配置的DBCs模型，
因此除了利用编码误差表选出
优先级更高的分区配置外，还需要基于编码误差表进行一定随机性的探索，
从而使得编码误差表中编码误差暂时较大的分区配置也机会得到使用。

因此与DeepSeekMoE\cite{dai2024deepseekmoe}将混合专家模型中的专家
分为共享专家与路由专家两部分类似，本章将分区配置分为
稳定分区配置与随机分区配置。每次迭代过程中，利用编码误差表
激活编码误差较小的分区配置作为稳定分区配置，在剩余分区配置的编码误差表中将编码误差作为概率，
随机采样出随机分区配置，两部分组合成为本次迭代中激活的分区配置。
为了实现根据编码误差的随机采样，使用了强化学习中常用的Gumbel-Max技巧\cite{jang2016categorical}。
Gumbel-Max技巧提供了一种简单而有效的方法，从分类概率为$\pi$的
类别分布中采样样本$z$：
\begin{equation}
z = \arg\max_i \left( \log\pi_i + g_i \right)
\end{equation}
其中$g_i$是从$Gumbel(0, 1)$分布中采样的噪声：
\begin{equation}
    g = -log(-log(u))
\end{equation}
其中$u$表示从均匀分布$Uniform(0, 1)$中采样的随机数，
$g$表示采样得到的Gumbel噪声。

综上所述，本章提出了一种基于混合专家模型的分区配置选择器，
称为MoP(Mixture of Partitions)。
MoP的具体过程如图\ref{fig:Mop}所示，其中Affinity Table为
分区配置的亲和度表，值为编码误差的倒数，亲和度越大则相应的编码误差越小。
每次迭代时，根据分区配置的亲和度表进行Top-K排序，
选择前$N_s$大的亲和度对应的编码配置作为稳定分区配置，
对于亲和度表中剩余分区配置通过Gumbel-Max技巧采样$N_r$个随机配置。
最终一共$N_s+N_r$个分区配置被激活，并进行可微编解码。
对于非分区配置，由于数量较少，因此每次迭代时都会进行计算，
相当于将非分区配置都作为稳定配置。当$N_s+N_r$等于分区配置总数时，
MoP的计算开销等于上一章中使用穷举法的DBC模型。

\begin{figure*}[htbp]
    \centering
    \includegraphics[width=0.7\textwidth]{figures/Mop_v3.pdf}
    \caption{基于混合专家模型的分区配置选择器(MoP)}
    \label{fig:Mop}
\end{figure*}

\begin{figure*}[htbp]
    \centering
    \includegraphics[width=1\textwidth]{figures/DBC_overview_v4.pdf}
    \caption{基于MoP的DBCM模型的整体框架}
    \label{fig:DBCM}
\end{figure*}

为了与上一章提出的DBC模型进行区分，本章提出的具有分区配置选择器
的可微块压缩模型称为DBCM模型。DBCM模型的整体框架如图\ref{fig:DBCM}所示。
神经纹理的每4\times4块的编码配置分为分区配置与非分区配置，
图中的MoP Configs表示通过MoP激活的分区配置，未激活的分区配置
则在此次迭代中不进行计算。神经纹理的每4\times4块使用MoP激活的分区配置
与非分区配置进行可微编解码后，使用配置选择器根据编码误差选择输出时
也使用了Gumbel-Max技巧，因此将DBCM的配置选择器
称为Gumbel配置选择器。与之前MoP中的Gumbel-Max技巧不同，
Gumbel配置选择器将Gumbel噪声乘温度项$\tau$，
随着温度的降低，Gumbel噪声逐渐减弱，Gumbel配置选择器
逐渐转变为上一章中的argmax配置选择器。
这样做的原因是神经纹理在优化过程中不断发生变化，
在优化早期使用的编码配置可能对优化后期的同一个纹理块
不再适用，因此类似模拟退火算法，在优化初期进行一定程度的探索，
从而更好地搜索到每个块最佳的编码配置。

\section{实现细节}

DeepSeekMoE\cite{dai2024deepseekmoe}中的
门控网络输出路由专家对输入的亲和度，并根据亲和度选择路由专家，
而纹理编解码过程只需要选择编码误差最小的编码配置，因此可以使用编码误差的倒数作为对分区配置的亲和度，
同时无需使用门控网络：
\begin{equation}
s_i=\frac{1}{L_i}
\end{equation}
其中$s_i$表示分区配置$i$的亲和度，
$L_i$表示使用分区配置$i$产生的压缩误差。

为了减少大量分区配置造成的计算开销，每次迭代时仅激活部分的分区配置，
因此分区配置的亲和度表中，激活的分区配置可以得到更新，
未激活的分区配置使用之前迭代中计算出的历史亲和度近似。
假设某次迭代选中的分区配置集合为$\{i|i\in\mathcal{I}^*\}$，更新亲和度表时：
\begin{equation}
    s_i\leftarrow
    \begin{cases} 
        (1-\alpha)s_i+\alpha\frac{1}{\|B-B'_i\|_F^2}, & \text{if } i\in\mathcal{I}^*,\\
    s_i, & \text{otherwise}.
    \end{cases}
\end{equation}
其中$B$表示可微编解码前的4\times4纹理块，$B'_i$表示$B$使用分区配置$i$可微编解码后的4\times4纹理块。
$\alpha$ 是滑动平均系数，固定为0.2，$\alpha$的作用是使得亲和度表
的更新更加平缓，防止随机分区配置具有的随机性导致亲和度表中的值变化过大，造成训练过程不稳定。

优化前需要对分区配置的亲和度表$\{s_i|i\in\mathcal{I}\}$进行初始化，
初始化方法为使用穷举法计算一次每4\times4块中所有分区配置对应的编码误差的倒数，将其作为
分区配置的亲和度。

亲和度表存储了每4\times4纹理块中每个分区配置对应的亲和度。
基于亲和度表可以通过Top-K方法选出优先级较高的稳定分区配置。
具体来说，首先根据亲和度表从所有分区配置集 $\mathcal{I}$ 中
选出亲和度最大的前$N_s$个分区配置作为稳定分区配置$\mathcal{I}_s$ ，即
\begin{equation}
\label{eq41}
\mathcal{I}_s=\left\{i|s_i\in\text{Topk}(\{s_i|j\in\mathcal{I}\},N_s)\right\}
\end{equation}
其中 $\text{Topk}(\mathcal{S},n)$ 表示从集合 $\mathcal{S}$ 中选出最大的 $n$ 个元素组成的集合，
$s_i$表示亲和度表中分区配置$i$的亲和度。

为了防止总是使用数量较少的稳定分区配置导致方法退化为固定配置。
与\inlinecite{shazeer2017outrageously}中的Noisy Top-K门控类似，
在对亲和度表进行Top-K计算时，通过增加噪声的方式，实现编码配置的随机采样，
从而尝试更多的分区配置。
因此使用强化学习中常用的Gumbel-Max技巧\cite{jang2016categorical}向\eqref{eq41}中添加了Gumbel噪声\cite{jang2016categorical}
实现将亲和度作为概率采样$N_r$个随机分区配置$\mathcal{I}_r$：
\begin{equation}
\mathcal{I}_r=\left\{i|s_i+g_j\in\text{Topk}(\{s_i+g_j|j\in\mathcal{I}-\mathcal{I}_s\},N_r)\right\}
\end{equation}
其中 $\text{Topk}(\mathcal{S},n)$ 表示从集合 $\mathcal{S}$ 中选出最大的 $n$ 个元素组成的集合
，$g_j$表示从$Gumbel(0,1)$分布中获得的噪声样本。

最终激活的分区配置集合$\mathcal{I}^*$为：
\begin{equation}
\mathcal{I}^*=\mathcal{I}_{s}\cup\mathcal{I}_r
\end{equation}
接下来激活的分区配置与非分区配置一起作为激活的编码配置，
通过可微编解码器对4\times4纹理块进行编解码。
最后使用Gumbel配置选择器选择出最佳配置：

设输入纹理块 $\mathbf{B}\in\mathbb{R}^{b\times n}$，其中 $b$ 是一个纹理块中的纹素个数。
基于混合专家模型选中的编码配置集合为 $\mathcal{I}^*$，$\mathbf{B}$使用配置 $i\in\mathcal{I}^*$ 
经过可微编解码后得到的纹理块为 $\mathbf{B}'_i\in\mathbb{R}^{b\times n}$，
对应的压缩误差为 $L_i$。
\begin{equation}
L_i=\|\mathbf{B}-\mathbf{B}'_i\|_F^2
\end{equation}
\begin{equation}
\mathbf{B}'_{i^*}=\sum_{i\in\mathcal{I}} w_i\mathbf{B}'_i
\end{equation}
\begin{equation}
    w_i =\left\{\begin{matrix}
        1,& i=\mathop{\arg\min}\limits_{i} (L_i+\tau*g_j)
        \\0,& i\ne\mathop{\arg\min}\limits_{i} (L_i+\tau*g_j)
        \end{matrix}\right.
\end{equation}
其中 $w_i$ 是配置 $i$ 的组合系数，由于最终只选择使得$L_i+\tau*g_j$最小的配置，
因此实际上 $w_i$ 是 one-hot 的，
$g_j$表示从$Gumbel(0,1)$分布中获得的噪声样本，
$\tau$表示温度系数，$\tau$较大时
噪声的权重较大，编码配置的选择过程接近随机采样，$\tau$越小，编码配置选择过程的随机性也越小。
当$\tau=0$时，Gumbel配置选择器等价于上一章中的配置选择器。
每次迭代过程中使用指数衰减的方式，令$\tau$不断减小，从而保证优化初期对编码配置的探索，
优化后期以编码误差最小的方式选择编码配置。

设所有分区配置集合$\mathcal{I}$中的配置数为$N$，
DBCM模型的计算量是原来的$\frac{N_s+N_r}{N}$，当$N_s+N_r=N$时计算量等于上一章中的DBC模型。
DBCM的优点包括：
\begin{itemize}
\item 稳定性：稳定分区配置保证总是会激活分区配置中编码误差较小的分区，
随机分区配置保证优化过程不会退化为固定分区配置。
这种方式与标准DXT格式编解码器相同，保证可微近似的正确性，同时使得训练过程比较稳定。
\item 探索性：Gumbel配置选择器保证对编码配置的探索，因为优化过程的神经纹理不断发生变化，
优化初期的编码配置不一定适合优化后期的神经纹理，因此在优化初期进行探索，优化后期逐渐固定到某一个配置的
模拟退火过程更容易找到全局最优的编码配置。
\item 效率：通过结合稳定分区配置与随机分区配置的选择策略，该模型在确保训练稳定性和探索性的同时可以显著提高计算效率。
\end{itemize}


\section{实验设计与分析}

本章将基于混合专家模型的分区配置选择器(MoP)加入
DBC模型，称为DBCM模型，并在神经压缩模型、
基于移动基分解的光照数据压缩模型
\cite{silvennoinen2021moving}（Moving Basis Decomposition，MBD）和
HDR 纹理的 RGBM 编码模型，三个应用场景中进行神经纹理的压缩实验。
神经压缩模型中，神经纹理作为神经特征进行训练。
MBD模型中，神经纹理作为插值基向量的系数进行训练。
RGBM编码模型中，神经纹理作为HDR纹理经过RGBM编码产生的LDR纹理进行训练。
实验过程中使用i7-13700KF和RTX 2080Ti进行测试。

\subsection{实验设置}

\paragraph{实验数据集}

在神经压缩模型的实验中，使用来自polyhaven.com\cite{PolyHaven}的 6 个公开的材质纹理集，
分别为Ukulele\_01、antique\_katana\_01、utility\_box\_02、chinese\_chandelier、
lubricant\_spray和Drill\_01。
每个纹理集包含 8 个通道，由 3 张纹理组成：3通道的漫反射纹理、2通道的法线纹理 和 3通道的ARM纹理（环境光遮蔽、粗糙度、金属度），
分辨率均为2048×2048(2K)。

在MBD的实验中，使用UE引擎烘焙的场景光照数据，该数据来自 5 个场景，
分别命名为 Cornell Box、Scene 1、Scene 2、Scene 3 和 Scene 4。
其中Cornell Box场景的光照数据使用二阶球谐函数存储，其他4个场景的光照数据
使用一阶球谐函数存储。

在RGBM的实验中，使用来自polyhaven.com\cite{PolyHaven}的 6 个公开的HDR图像，
分别为lakeside\_sunrise、mirrored\_hall、stierberg\_sunrise、
studio\_garden、urban\_street和vignaioli\_night，
分辨率均为1024×1024(1K)，并将HDR图像的最大值限制在32以内。

\paragraph{标准DXT格式编解码器}

本章也使用英伟达开发的纹理压缩工具NVTT3\cite{NVTT3}(NVIDIA Texture Tools 3)，
作为标准DXT格式编解码器。
结合了DBCM模型的神经纹理优化结束后，
需要使用NVTT3对神经纹理进行DXT格式压缩，
以导出DXT格式进行测试，从而验证DBCM模型的有效性。

\paragraph{评估指标}

神经压缩模型与RGBM编码模型的实验使用
峰值信噪比(Peak Signal-to-Noise Ratio, PSNR)、
结构相似性\cite{wang2004image}(Structural Similarity, SSIM)
以及FLIP\cite{andersson2020flip}进行评估。
MBD模型的实验对于光照数据本身的压缩损失使用均方根误差
（Root Mean Squared Error，RMSE）进行评估，对于使用光照数据
渲染的场景图像使用PSNR与FLIP进行评估。

\paragraph{对比方法}

通过本章提出的分区配置选择器改进的DBC模型称为DBCM模型。
在神经压缩实验中使用以下方法进行对比：

\begin{itemize}
    \item BC6，BC7： 训练过程不使用DBC模型。
    \item DBC6s，DBC7s：： 训练过程使用DBC模型，但使用固定的编码配置。
    \item DBC6，DBC7： 训练过程使用上一章提出的编码配置动态选择的DBC模型。    
\end{itemize}

这些方法在实验测试时都会使用NVTT3将神经纹理导出为BC6/BC7格式进行测试。
MBD光照数据压缩实验与RGBM编码实验由于神经纹理为RGBA格式，因此
使用DBC7M模型进行实验，MBD与RGBM编码实验仅验证DBCM模型的有效性，因此这两个实验中
DBCM模型仅与使用BC7直接压缩神经纹理对比。

\paragraph{实现细节}

DBCM模型中的$N_s$与$N_r$都设为2，以兼顾计算效率与编码质量。
亲和度表更新的滑动平均系数$\alpha$固定为0.2。
Gumbel配置选择器的$\tau$初始化为100，每次迭代乘0.9965521，
通过指数衰减的方式在2000次迭代降至1e-4。

神经压缩实验的具体配置与上一章相同，
使用4个大小分别为512x512、256x256、128x128、64x64的特征网格，对于DBCM模型的可微BC7(DBC7M)使用
4通道特征网格，对于DBCM模型的可微BC6(DBC6M)使用3通道特征网格。
神经网络使用两层全连接结构，隐藏层通道数为16，输入通道数为12，输出通道数为8（纹理集中一组纹理的通道数之和）。
神经压缩模型中的神经纹理初始化为-1到1之间的均匀分布，神经网络权重和偏置项使用默认初始化。
将学习率初始化为0.01，
使用Adam优化器先进行10000次迭代的预训练，预训练过程不对神经纹理使用任何压缩方法。
预训练完成后，重置Adam优化器并接入DBC6M/DBC7M模型继续训练。
训练过程使用学习率衰减策略，
若连续 40 次迭代训练损失不降，将学习率乘衰减系数 0.9。
每隔 50 次迭代使用NVTT3导出BC6/BC7格式的神经纹理进行测试，并计算真实损失，
保存真实损失最小的神经纹理与网络权重。
当连续 5 次（对应 250 次迭代）真实损失不降则停止训练。

MBD实验的学习率初始化为0.1，训练过程也使用Adam优化器。
首先对场景里通过球谐函数存储的光照数据进行PCA，
从而初始化系数与基向量，然后进行2048次迭代的预训练。
预训练完成后，重置Adam优化器并将MBD方法作为上游模型接入DBC6M/DBC7M模型继续训练。
训练过程使用的学习率衰减策略与停止策略与神经压缩实验相同。

RGBM编码实验中由于RGBM编码模型本身无需训练，
因此不进行预训练。
将RGBM编码算法\ref{algo:RGBMEncode}中的Multiplier参数设为32，
然后对6个HDR图像进行RGBM编码，使用编码得到的LDR图像初始化神经纹理。
初始化完成后，使用Adam优化器并将学习率设为0.01，
将RGBM解码过程作为上游模型接入DBC6M/DBC7M模型对神经纹理进行训练。
训练过程使用的学习率衰减策略与停止策略与神经压缩实验相同。

三个实验使用的损失函数都为均方误差(Mean Squared Error, MSE)。
对于三个实验中使用的DBC7M方法，都需要加入
上一章中的范围约束损失，原因是需要将三个实验中的神经纹理
约束在BC7格式支持的范围，范围约束损失的权重$\lambda$固定为0.1。
三个实验中的神经纹理训练完成后都使用NVTT3导出为BC6/BC7格式进行测试。

\subsection{实验结果与分析}

\subsubsection{对比实验}
\label{4:对比实验}

神经压缩实验的实验结果如表\ref{6个材质的纹理集上进行的对比实验}所示，
相较于上一章的DBC模型，改进的DBCM模型6组实验的结果都好于DBC模型。
实验结果证明本章提出的分区配置选择器使用较少的计算开销就能
达到穷举所有分区配置的效果。
同时DBCM模型使用了退火的Gumbel配置选择器，这使得在优化初期可以对编码配置
进行一定程度的探索，同时也保证优化末期神经纹理的每个4\times4块可以选到编码误差最小的配置。
其次DBCM模型在亲和度表的更新过程中加入了$\alpha=0.2$的滑动平均，
使每次优化过程更稳定，不容易在两种编码配置之间来回跳变。
这些改进使得DBCM模型获得了相比DBC模型更好的实验结果。

\begin{table*}[htbp]
    \centering
    \caption{6个材质的纹理集上进行的对比实验结果}
    \label{6个材质的纹理集上进行的对比实验}        
    \resizebox{\linewidth}{!}{
    \begin{tabular}{ccccccccccccccccccc}
        \toprule
        & \multicolumn{3}{c}{Ukulele\_01} & \multicolumn{3}{c}{antique\_katana\_01} & \multicolumn{3}{c}{utility\_box\_02} & \multicolumn{3}{c}{chinese\_chandelier} & \multicolumn{3}{c}{lubricant\_spray} & \multicolumn{3}{c}{Drill\_01}\\
                  & PSNR(↑)  & SSIM(↑) & FLIP(↓)& PSNR(↑) & SSIM(↑) & FLIP(↓)& PSNR(↑) & SSIM(↑) & FLIP(↓) & PSNR(↑) & SSIM(↑)& FLIP(↓) & PSNR(↑)& SSIM(↑)& FLIP(↓)& PSNR(↑) & SSIM(↑)& FLIP(↓)\\
        \midrule
        BC6       & 22.3005  & 0.6716  & 0.2713 & 24.1197 & 0.7007  & 0.2161 & 23.8151 & 0.6977  & 0.2341  & 20.1739 & 0.604  & 0.3457 & 22.6572 & 0.6651 & 0.2726 & 22.6281 & 0.7039 & 0.2976  \\
        DBC6s     & 27.1085  & 0.7625  & 0.1705 & 27.8752 & 0.8197  & 0.1332 & 28.027  & 0.7853  & 0.1472  & 25.7551 & 0.7407 & 0.2197 & 26.0462 & 0.7785 & 0.1846 & 26.8157 & 0.8328 & 0.163  \\
        DBC6      & 27.083   & 0.7622  & 0.1707 & 27.9227 & 0.8233  & 0.1322 & 28.0362 & 0.7877  & 0.1453  & 25.8831 & 0.7453 & 0.2163 & 26.0767 & 0.775  & 0.1848 & 26.8316 & 0.8295  & 0.1693   \\
        DBC6M     & \textbf{28.271}   & \textbf{0.7828}  & \textbf{0.1464} & \textbf{28.9463} & \textbf{0.8432}  & \textbf{0.1134} & \textbf{28.7504} & \textbf{0.8018}  & \textbf{0.1326}  & \textbf{26.8738} & \textbf{0.7626} & \textbf{0.1942} & \textbf{26.8452} & \textbf{0.8048} & \textbf{0.161} & \textbf{28.2355} & \textbf{0.8549} & \textbf{0.1394}  \\
        \midrule
        BC7       & 23.0234  & 0.6685  & 0.2606 & 25.0525 & 0.6602  & 0.2262 & 25.0392 & 0.7163  & 0.2338  & 22.45   & 0.6358 & 0.3009 & 21.4505 & 0.6852 & 0.2457 & 22.2064 & 0.7357 & 0.2971   \\
        DBC7s     & 30.2512  & 0.8221  & 0.1087 & 30.602  & 0.8683  & 0.0919 & 30.8006 & 0.8338  & 0.1068  & 29.6132 & 0.8223 & 0.1378 & 29.5706 & 0.8618 & 0.0949 & 31.8172 & 0.9084 & 0.0986  \\
        DBC7      & 30.534   & 0.8277  & 0.1018 & 30.8209 & 0.8736  & 0.0871 & 31.154  & 0.8412  & 0.0998  & 29.9245 & 0.8303 & 0.1304 & 29.7846 & 0.8666 & 0.0905 & 32.2309 & 0.9156 & 0.0893   \\
        DBC7M     & \textbf{30.633}   & \textbf{0.8304}  & \textbf{0.0966} & \textbf{31.4472} & \textbf{0.8917}  & \textbf{0.0761} & \textbf{31.2531} & \textbf{0.844}   & \textbf{0.096}   & \textbf{30.0176} & \textbf{0.8314} & \textbf{0.1273} & \textbf{29.9894} & \textbf{0.8709} & \textbf{0.0865} & \textbf{33.6313} & \textbf{0.9326} & \textbf{0.0752}  \\
        \bottomrule
\end{tabular} }   
\end{table*}

MBD实验的结果如表\ref{MBDexp1}所示，
该表列出了5个场景中不同方法的RMSE和存储大小的具体值。
其中结合了DBC7M模型的MBD方法的RMSE明显低于
直接使用BC7，同时占用的存储空间大小仅为MBD方法的1/4。
实验结果证明向MBD中加入DBC7M模型并进行联合优化有效降低
了对MBD的插值系数进行BC7格式压缩造成的光照数据重建损失

\begin{table*}[htbp]
    \centering
    \caption{5个烘焙的场景光照数据上进行的对比实验结果}
    \label{MBDexp1}        
    \resizebox{\linewidth}{!}{
    \begin{tabular}{lllllllllll}
        \toprule
                    & \multicolumn{2}{l}{Cornell Box} & \multicolumn{2}{l}{Scene 0} & \multicolumn{2}{l}{Scene 1} & \multicolumn{2}{l}{Scene 2} & \multicolumn{2}{l}{Scene 3} \\
                    & RMSE       & Memory Size / KB   & RMSE     & Memory Size / KB & RMSE     & Memory Size / MB & RMSE     & Memory Size / MB & RMSE     & Memory Size / MB \\
        \midrule
    MBD + BC7       & 0.427069   & 4                  & 0.312421 & 96.25            & 0.462323 & 0.29             & 0.141934 & 0.38             & 0.415555 & 1.24             \\
    MBD + DBC7M     & 0.183869   & 4                  & 0.150063 & 96.25            & 0.292717 & 0.29             & 0.114637 & 0.38             & 0.376664 & 1.24             \\
    MBD             & 0.078967   & 16                 & 0.042723 & 385              & 0.071571 & 1.15             & 0.065373 & 1.53             & 0.198106 & 4.96             \\
    Reference       & -          & 432                & -        & 4,620            & -        & 14.13            & -        & 18.47            & -        & 59.55           \\
\bottomrule
\end{tabular} }   
\end{table*}

\begin{figure*}[htbp]
    \centering
    \includegraphics[width=1.0\linewidth]{figures/MBD_v3.pdf}
    \caption{两个场景光照数据的渲染结果}
    \label{fig:MBD_v3} 
\end{figure*}

使用其中两个场景的光照数据进行了渲染，结果如图\ref{fig:MBD_v3}所示，
其中第一行是Cornell Box场景，第二行是Scene 2场景。
每一列表示不同的方法，最右侧是使用无损光照数据渲染的结果。
实验结果与表\ref{MBDexp1}相符，
直接使用BC7导致渲染结果显示出明显的伪影，
结合了DBC7M模型后很大程度减轻了渲染结果中的伪影，
视觉效果上非常接近使用无损的光照数据渲染的结果。

RGBM编码的实验结果如表\ref{tb:RGBM}所示，
6组实验中使用了DBC7M模型的方法相比直接使用BC7都取得了更好的结果，
证明DBC7M模型有效降低了BC7压缩造成的HDR重建误差。

对studio\_garden与vignaioli\_night进行了局部放大的可视化便于进行细节的比较，
结果如图\ref{fig:RGBM}所示。
其中第一行是studio\_garden，第二行是vignaioli\_night。
每一列表示不同的方法，最右侧是无损的参考图像。
与表\ref{tb:RGBM}中的结果相符，
直接使用BC7造成了严重的块状伪影，
结合了DBC7M模型后很大程度减轻了重建的HDR图像中的伪影，
视觉效果上非常接近无损的参考图像。

\begin{table*}[htbp]
    % \begin{minipage}{\columnwidth}
    \caption{6个HDR图像上进行的对比实验结果}
    \label{tb:RGBM}
    \begin{center}
    \begin{tabular}{ccccccccccccccccccc}
    \toprule
                    & \multicolumn{3}{c}{RGBM+BC7}  & \multicolumn{3}{c}{RGBM+DBC7M}  \\ 
                    & PSNR(↑)   & SSIM(↑) & FLIP(↓)&PSNR(↑)& SSIM(↑) & FLIP(↓) \\
    \midrule
lakeside\_sunrise    & 37.4817   & 0.9666 & 0.0679 &44.1479&0.986&0.0607 \\
mirrored\_hall       & 35.3975   & 0.9148 & 0.0814 &40.5526&0.9832&0.0539 \\
stierberg\_sunrise   & 34.8078   & 0.9457 & 0.0621 &42.5311&0.9868&0.0433 \\
studio\_garden       & 35.7492   & 0.9561 & 0.1014 &44.8351&0.9918&0.0909 \\
urban\_street        & 38.7993   & 0.9748 & 0.0586 &47.4841&0.992&0.0548 \\
vignaioli\_night     & 38.3124   & 0.9814 & 0.0721 &43.5846&0.9942&0.0607 \\
    \bottomrule
    \end{tabular}
    \end{center}
    % \end{minipage}
\end{table*}

\begin{figure*}[htbp]
    \centering
    \includegraphics[width=1.0\linewidth]{figures/RGBM.pdf}
    \caption{studio\_garden与vignaioli\_night的HDR重建结果}
    \label{fig:RGBM} 
\end{figure*}

三个应用场景中的对比实验证明了DBCM模型的有效性与通用性。

\subsubsection{性能测试}

在神经压缩实验中6个材质的纹理集上进行了不同方法的性能测试，
其中DBCM模型的$N_s$与$N_r$都设为2。实验结果取6次实验的平均值，
如表\ref{4:6个材质的纹理集上进行的性能实验}所示。
DBCM模型由于使用了较少的编码配置数，
相比上一章通过穷举法实现编码配置动态选择的DBC模型
运行速度显著提高，其中DBC7M与DBC7相比速度提升了一个数量级，
与固定编码配置的DBC7s相比耗时仅增加了一倍，
由于DBC7s本身的运行速度较快，
因此DBC7M的运行时间相比DBC7s仅少量增加。
而DBC6M与DBC6相比速度同样显著提升，与固定编码配置的DBC6s相比
耗时也仅增加一倍，
由于DBC6s本身的运行速度相比DBC7s更快，
因此DBC6M的运行时间相比DBC6s的增加并不明显。
性能测试证明了本章提出的
分区配置选择器的有效性，实现了在几乎不损失编码质量的前提下，
显著提升编码速度的目标。

\begin{table*}[htbp]
    \centering
    \caption{6个材质的纹理集上进行的性能测试结果}
    \label{4:6个材质的纹理集上进行的性能实验}        
    \resizebox{0.75\linewidth}{!}{
    \begin{tabular}{ccccccccccccccccccc}
        \toprule
                                    & BC6 & DBC6s & DBC6 &DBC6M& BC7  & DBC7s & DBC7 &DBC7M\\
        \midrule
        每次迭代的平均运行时间(ms)  & 33  &  80   & 1012  & 166     & 35   & 171    & 2938 &  278 \\
        优化完成的平均迭代次数      & -   &  2200  & 2083 & 1808     & -   & 3750   &  4416    &4625 \\
        优化完成的平均时间(min)     & -   &  3     & 35  &  5        & -   & 11      &  216  &21\\
        \bottomrule
\end{tabular} } 
\end{table*}

\subsubsection{参数敏感性实验}

DBCM模型中的分区配置选择器具有$N_s$与$N_r$
两个参数，为了测试$N_s$与$N_r$的参数敏感性，
使用不同的$N_s$与$N_r$组合在神经压缩模型中进行了实验，
实验使用 6 个材质纹理集中的lubricant\_spray。
除$N_s$与$N_r$外，其余实验设置与之前的神经压缩实验相同。
实验结果如表\ref{tb:NsNr}所示。
% 其中最右边的$N_s=64$， $N_r=0$计算量相当于上一章的DBC模型。
$N_s$与$N_r$之和相等时，
$N_s=N_r$的结果最好，其次是$N_s>N_r$，
最后是$N_r>N_s$。
$N_s>N_r$的结果比$N_r>N_s$的结果更好证明了根据
亲和度表的TopK排序选择稳定分区配置的有效性。
而$N_s=N_r$的结果比$N_s>N_r$的结果更好证明了
在稳定分区配置的基础上，加入基于亲和度表进行随机采样的
随机分区配置的有效性，这种设置兼顾了对亲和度表的探索与利用，
因此取得了最好的结果。
只使用随机分区配置的设置($N_s=0$)与只使用稳定分区配置的设置($N_r=0$)
实验结果随着分区配置的总数增加逐渐变好，原因是对于这两种设置，
分区配置总数越多则越接近穷举所有分区配置的方法，因此效果也会变好。
$N_s=4$，$N_r=4$的结果相比$N_s=2$，$N_r=2$稍差，
原因是引入稳定分区配置与随机分区配置后，
随机性的增加使得模型更加复杂，因此优化效果与分区配置总数不再成正相关。

\begin{table*}[htbp]
    \centering
    \caption{ $N_s$与$N_r$的参数敏感性实验结果}
    \label{tb:NsNr}
    \resizebox{\linewidth}{!}{
    \begin{tabular}{ccccccccccccccccccc}
    \toprule
    $N_s$ / $N_r$ & 0 / 2 & 1 / 1 & 2 / 0 & 0 / 4 & 2 / 2 & 4 / 0 & 0 / 8 & 4 / 4 & 8 / 0 & 64 / 0 \\
    \midrule
    PSNR (dB)     & 29.5630 & \textbf{29.9851} & 29.8575 & 29.7333 & \textbf{30.0586} & 29.9351 & 29.8061 & \textbf{30.0223} & 30.0067 & 30.0684 \\
    SSIM          & 0.8708  & \textbf{0.8737}  & 0.8701  & 0.8721  & \textbf{0.8740}  & 0.8719  & 0.8739  & \textbf{0.8741}  & 0.8740  & 0.8752  \\
    FLIP          & 0.0937  & \textbf{0.0908}  & 0.0933  & 0.0922  & \textbf{0.0903}  & 0.0922  & 0.0925  & \textbf{0.0900}  & 0.0901  & 0.0899  \\
    \bottomrule
    \end{tabular}}
\end{table*}

\section{本章小结}

本章提出了一种基于混合专家模型的分区配置选择器，
相比固定编码配置的DBCs模型，仅增加少量计算开销就能
接近穷举所有编码配置的DBC模型的效果。
引言部分阐述了DBC模型穷举所有编码配置
造成大量计算开销的问题，分析了BC6与BC7的编码配置中，
分区配置占比最高的特点，确定了从优化分区配置选择的角度减少计算开销
的优化思路。
然后阐述了基于穷举法进行编码配置选择过程造成的计算量浪费问题，
并提出了一种基于混合专家模型的分区配置选择器(MoP)，
以及基于MoP改进的DBCM模型的整体框架。
之后介绍了MoP的具体实现方式。
最后在神经压缩、MBD光照数据压缩、RGBM编码
三个应用场景中对本章所提出的DBCM模型进行测试，
以证明本章所提出的方法的有效性。