% !TeX root = ../main.tex

\chapter{基于混合专家模型的编码配置选择器}

\section{引言}

上一章提出的可微纹理块压缩模型中，为了在多种编码配置中选择最佳配置，
采用了与标准DXT格式编解码器类似的方法，即在可微编解码过程中使用所有编码配置，
并根据每种编码配置的压缩误差选择最佳编码配置。这种方法简单并且易于实现，
但会造成显著的计算开销。为了优化这种编码配置的计算效率，本章提出了一种基于混合专家模型的编码配置选择器，
这个模块将替换上一章中的配置选择器，在较少压缩质量损失的前提下，提高可微编解码器的计算效率。

\section{问题描述}

对于DXT格式中的多模式格式（例如BC6与BC7），对于纹理中的每4x4纹理块，编码时都会选择
其中一种编码配置进行编码（例如某种特定的分区操作或将RGB通道之一与A通道交换的旋转操作等）。
在标准DXT格式编解码器中，为了确定最佳的编码配置，通常会使用多种编码配置进行编解码，并选择使得压缩误差最低的
编码配置作为最优配置。对于面向压缩任务的标准编解码器，这个过程对于某个纹理块只进行一次，因此带来的计算开销可以被接受。
但对于本文提出的面向优化任务的可微编解码器，这个过程会在优化过程中每次迭代进行一次，因此带来的计算开销比较显著。
例如BC6具有单分区与32种双分区共33种不同配置，BC7具有4种旋转通道的单分区以及64种双分区共68种不同配置，
计算所有编码配置意味着对于BC6需要将计算量增加至33倍，对于BC7是68倍。
为解决这一问题，本章提出了一种基于混合专家模型的编码配置选择器。

\section{模型框架}

最初的混合专家模型\inlinecite{jacobs1991adaptive}中，每个专家都代表一个前馈神经网络，
还有一个门控网络用于选择使用哪个专家的结果作为输出。专家网络和门控网络都接收相同的输入，
门控网络将输入乘上一个可训练的权重矩阵后，使用softmax层获得每个专家的概率分布，
最后根据这个概率分布在所有专家中随机采样其中一个并作为输出。
最近的DeepSeekMoE\cite{dai2024deepseekmoe}将一部分专家隔离为共享专家，其他专家被称为路由专家。
共享专家的输出总是有效。路由专家的输出通过门控网络输出的概率控制，
其中概率最高的前k个路由专家的输出有效，然后通过概率对路由专家的输出进行加权混合。
最终的输出为将共享专家与加权后的有效路由专家两部分输出之和。

纹理编解码过程中，一种编码配置可以视作一个专家，最终会在多种编码配置中选择一个进行编码，
这个过程与最初的混合专家模型中通过门控网络选择一个专家输出的结构类似。混合专家模型中的
门控网络将输入通过可学习的线性层与softmax层获得概率分布，然后根据概率分布选择专家，
而纹理编解码过程只需要选择压缩误差最小的编码配置，因此可以将门控网络替换为压缩误差表，
表中存储每4x4块中每个编码配置对应的压缩误差，编码时根据压缩误差表选择前k个最佳配置，
首次迭代前初始化整个压缩误差表，每次迭代更新压缩误差表。由于每个纹理块仅需要一个编码配置，
对于选出的前k个配置，还需要分别进行编解码，最后根据编码误差选择其中最好的一个编码配置。

设所有配置的集合为 $\mathcal{I}$，需要计算的配置集合为 $\mathcal{I}^*\subseteq \mathcal{I}$。为了保证计算效率，我们不会在每个epoch中计算所有配置，而是只计算一部分配置，因此在此对两者进行区分。

设输入纹理块 $\mathbf{B}\in\mathbb{R}^{b\times n}$，其中 $b$ 是一个纹理块中的纹素个数。针对配置 $i\in\mathcal{I}$ 经过可微编解码后得到的压缩纹理块为 $\mathbf{B}'_i\in\mathbb{R}^{b\times n}$，由此整合出的纹理块 $\mathbf{B}^*\in\mathbb{R}^{b\times n}$ 表示为
$$
\mathbf{B}^*=\sum_{i\in\mathcal{I}^*} w_i\mathbf{B}'_i
$$
其中 $w_i$ 是配置 $i$ 的组合系数。由于最终我们只能选择一个配置，因此实际上 $\{w_i|i\in\mathcal{I}\}$ 是 one-hot 编码，即满足 $\sum_{i\in\mathcal{I}}w_i=1$ 和 $\sum_{i\in\mathcal{I}}w_i^2=1$。下文将介绍如何计算 $w_i$。

首先，定义压缩误差 $L_i$ 为
$$
L_i=\|\mathbf{B}-\mathbf{B}'_i\|_F^2
$$
定义配置 $i$ 的亲和度 $s_i$ 为
$$
s_i=\frac{1}{L_i}
$$
我们将使用 Gumbel trick 来实现对配置的随机选择。具体来说，令 $\{g_i|i\in\mathcal{I}^*\}$ 是 $\text{Gumbel}(0,1)$ 的独立样本集合，则配置 $i$ 的亲和度 $s_i$ 是集合 $\{s_j/\tau+g_j|j\in\mathcal{I}^*\}$ 中的最大元素的概率为
$$
\text{Pr}(i=\arg\max_{j\in\mathcal{I}^*}(s_j/\tau+g_j))=\frac{\exp(s_i/\tau)}{\sum_{j\in\mathcal{I}^*} \exp(s_j/\tau)}
$$

其中温度参数 $\tau>0$ 用于控制亲和度强度。显然，在 $\{s_i|i\in\mathcal{I}\}$ 固定且各不相同的情况下，有
$$
\lim_{\tau\to 0^+}\text{Pr}\left(\arg\max_{j\in\mathcal{I}^*}(s_j)=\arg\max_{k\in\mathcal{I}^*}(s_k/\tau+g_k)\right)=1
$$
即 $\tau$ 越接近 0，配置选择的随机程度越小，反之亦然。

最后，定义组合系数 $w_i$ 为
$$
w_i=\mathbb{1}(i=\arg\max_{j\in\mathcal{I}^*}(s_j+g_j))
$$

其中 $\mathbb{1}(\cdot)$ 是示性函数。

如果在每个 epoch 对所有分区配置进行计算，这将带来较大的计算开销。受到 DeepSeekMoE 框架的启发，我们提出了 MoP 框架，如图\ref{fig:Mop}所示，这可以看作是 DeepSeekMoE 在分区混合问题上的迁移应用。该框架在不影响训练效果的情况下显著提升了计算效率。

\begin{figure}[htbp]
    \centering
    \includegraphics[width=1\textwidth]{figures/Mop_v2.pdf}
    \caption{基于混合专家模型的编码配置选择器}
    \label{fig:Mop}
\end{figure}

具体来说，首先我们会根据亲和度从分区配置集合 $\mathcal{I}_p\subseteq \mathcal{I}$ 中选出最高的 $N_s$ 个分区配置作为稳定分区配置 $\mathcal{I}_p^s$ ，即
$$
\mathcal{I}_p^s=\left\{i|s_i\in\text{Topk}(\{s_j|j\in\mathcal{I}\},N_s)\right\}
$$
其中 $\text{Topk}(\mathcal{S},n)$ 表示从集合 $\mathcal{S}$ 中选出最大的 $n$ 个元素组成的集合。

接着再根据亲和度随机选出剩余分区配置中的 $N_r$ 个分区配置作为随机分区配置 $\mathcal{I}_{p}^r$，这里同样使用了 gumbel trick 来实现对分区配置的随机选择，即
$$
\mathcal{I}_p^s=\left\{i|s_i+g'_i\in\text{Topk}(\{s_j+g'_j|j\in\mathcal{I}_p\setminus\mathcal{I}_p^s\},N_r)\right\}
$$
其中 $\{g'_i|i\in\mathcal{I}^*\}$ 是 $\text{Gumbel}(0,1)$ 的独立样本集合。记这两种方式选出的分区配置为混合分区配置 $\mathcal{I}_p^m$，即
$$
\mathcal{I}_p^m=\text{MoP}(\mathcal{I}_p,N_s,N_r)=\mathcal{I}_p^s\cup\mathcal{I}_p^r
$$
则需要计算的配置集合 $\mathcal{I}^*$ 为
$$
\mathcal{I}^*=\mathcal{I}_{np}\cup\mathcal{I}_p^m
$$
此外，亲和度是用压缩误差定义的，而压缩误差的计算又需要使用到压缩的纹理块。但 MoP 框架需要在计算压缩的纹理块之前就根据亲和度来确定需要计算的分区配置。为了打破这个循环依赖问题，MoP 框架中引入了一个亲和度表 $\{s_i|i\in\mathcal{I}_p\}$ ，其记录了各分区配置最近一次计算的亲和度。在选择分区配置时使用该亲和度表，对于被选出的分区配置，计算压缩的纹理块 $\{B'_i|i\in\mathcal{I}_p^m\}$ 后，需要更新亲和度表，即
$$
s_i\leftarrow
\begin{cases} 
\frac{1}{\|B_i-B'_i\|_F^2}, & \text{if } i\in\mathcal{I}_p^m,\\
s_i, & \text{otherwise}.
\end{cases}
$$
因此，随机分区配置其实还起到了随着训练迭代不断更新亲和度表的作用。此外，在训练开始时需要先进行一次所有分区配置的计算来初始化亲和度表。

综上所述，MoP 框架的设计优点包括：

1. **稳定性**：稳定分区配置保证了每个 epoch 中我们总是会计算亲和度较高的分区配置，从而保证了训练的稳定性。
2. **探索性**：随机分区配置保证了每个 epoch 中我们总是会探索剩余的分区配置，因为即使有些分区配置在某个 epoch 中亲和度不高，但随着训练的进行，这些分区配置的亲和度可能会变高。
3. **效率**：通过结合稳定分区配置与随机分区配置的选择策略，MoP 框架在确保训练稳定性和探索性的同时，使计算效率提高了一个数量级。

\section{实验设计与分析}


