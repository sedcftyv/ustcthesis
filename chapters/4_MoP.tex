% !TeX root = ../main.tex

\chapter{基于混合专家模型的编码配置选择器}

\section{引言}

上一章提出的可微块压缩模型中，为了在多种编码配置中选择最佳配置，
采用了与标准DXT格式编解码器类似的方法，即在可微编解码过程中尝试所有编码配置，
并根据每种编码配置的压缩误差选择最佳编码配置。这种方法简单并且易于实现，
但会造成显著的计算开销。为了优化选择编码配置过程的计算效率，
本章提出了一种基于混合专家模型的编码配置选择器，
这个模块将替换上一章中的配置选择器，
在编码质量几乎不损失的前提下，提高DBC模型的计算效率。

\section{问题描述}

对于DXT格式中的多模式格式（例如BC6与BC7），对于纹理中的每4x4纹理块编码时都会选择
其中一种编码配置进行编码（例如不同的颜色端点与权重量化位数、
某种特定的分区操作或将RGB通道之一与A通道交换的旋转操作等）。
在标准DXT格式编解码器中，为了确定最佳的编码配置，通常会使用多种编码配置进行编解码，并选择使得压缩误差最低的
编码配置作为最优配置。对于面向压缩任务的标准编解码器，
这个过程对于某个纹理块只进行一次，因此带来的计算开销可以被接受。
但对于本文提出的面向优化任务的可微编解码器，这个过程会在优化过程中每次迭代进行一次，
因此带来的计算开销比较显著。
例如上一章中的DBC6在每次迭代过程进行可微编解码时，需要计算32种双分区$+$1种单分区共33种编码配置，
DBC7需要计算64种双分区$+$1种单分区$+$4种旋转\times3种不同量化方案共77中编码配置，
相较于仅计算一种编码配置的固定配置DBC模型，显著增加了运行时间。
为解决这一问题，本章提出了一种基于混合专家模型的编码配置选择器。

\section{模型框架}

最初的混合专家模型\inlinecite{jacobs1991adaptive}中，每个专家都代表一个前馈神经网络，
还有一个门控网络用于选择使用哪个专家的结果作为输出。专家网络和门控网络都接收相同的输入，
门控网络将输入乘上一个可训练的权重矩阵后，使用softmax层获得每个专家的概率分布，
最后根据这个概率分布在所有专家中随机采样其中一个并作为输出。
最近的DeepSeekMoE\cite{dai2024deepseekmoe}将一部分专家隔离为共享专家，其他专家被称为路由专家。
共享专家的输出总是有效。路由专家通过门控网络控制，门控网络的输出被称为亲和度(affinity)，
亲和度最高的前k个路由专家的输出有效，然后通过亲和度对有效的路由专家的输出进行加权混合。
最终的输出为将共享专家与加权后的有效路由专家两部分输出之和。

纹理编解码过程中一种编码配置可以视作一个专家，
DeepSeekMoE\cite{dai2024deepseekmoe}混合多个专家的结果，
但纹理编解码对于每个纹理块仅用一种编码配置，因此可以通过混合专家模型，
选出优先级较高的编码配置（专家），再在这些编码配置中选出最佳的一个，从而
避免上一章中计算所有编码配置带来的较大计算开销。

DeepSeekMoE\cite{dai2024deepseekmoe}中的
门控网络输出路由专家对输入的亲和度，并根据亲和度选择路由专家，
而纹理编解码过程只需要选择压缩误差最小的编码配置，因此可以使用压缩误差的倒数作为对编码配置的亲和度，
同时无需使用门控网络：
\begin{equation}
s_i=\frac{1}{L_i}
\end{equation}
其中$s_i$表示编码配置$i$的亲和度，
$L_i$表示使用编码配置$i$产生的压缩误差。

由于优化过程迭代进行，每次优化时数据发生的变化较小，可以使用之前迭代中计算出的亲和度
近似当前迭代的亲和度。即优化前初始化一个亲和度表$\{s_i|i\in\mathcal{I}\}$，
记录每种编码配置的亲和度，并在每次迭代中根据当前迭代选中的编码配置计算出的亲和度更新亲和度表。

假设某次迭代选中的编码配置集合为$\{i|i\in\mathcal{I}^*\}$，更新亲和度表时：
\begin{equation}
    s_i\leftarrow
    \begin{cases} 
    \frac{1}{\|B-B'_i\|_F^2}, & \text{if } i\in\mathcal{I}^*,\\
    s_i, & \text{otherwise}.
    \end{cases}
\end{equation}
其中$B$表示可微编解码前的4x4纹理块，$B'_i$表示$B$使用编码配置$i$可微编解码后的4x4纹理块。

亲和度表存储了每4x4纹理块中每个编码配置对应的亲和度。
基于亲和度表可以通过Topk方法选出优先级较高的编码配置。
% 亲和度表存储每4x4块中每个编码配置对应的亲和度，编码时根据亲和度表选择前$k$个最佳配置，
% 首次迭代前初始化整个压缩误差表，每次迭代更新亲和度表。由于每个纹理块仅需要一个编码配置，
% 对于选出的前$k$个配置，还需要分别进行编解码，最后根据编码误差选择其中最好的一个编码配置。
具体来说，首先根据亲和度表从所有编码配置集 $\mathcal{I}$ 中
选出亲和度最大的前$k$个编码配置$\mathcal{I}^*$ ，即
\begin{equation}
\label{eq41}
\mathcal{I}^*=\left\{i|s_i\in\text{Topk}(\{s_i|j\in\mathcal{I}\},k)\right\}
\end{equation}
其中 $\text{Topk}(\mathcal{S},n)$ 表示从集合 $\mathcal{S}$ 中选出最大的 $n$ 个元素组成的集合，
$s_i$表示亲和度表中编码配置$i$的亲和度。

该方法使用的亲和度表仅根据历史迭代数据计算，这种近似
导致所使用的亲和度表与当前迭代的真实值有一定差异。

与\inlinecite{shazeer2017outrageously}中的Noisy Top-K门控类似，可以
在对亲和度表进行Topk计算时，通过增加噪声的方式，实现编码配置的随机采样，
从而在优化初期尝试更多的编码配置，降低亲和度表近似产生的误差。
因此向\eqref{eq41}中添加了Gumbel噪声\cite{jang2016categorical}实现将亲和度作为概率采样编码配置：
\begin{equation}
\mathcal{I}^*=\left\{i|s_i+\tau*g_j\in\text{Topk}(\{s_i+\tau*g_j|j\in\mathcal{I}\},k)\right\}
\end{equation}
其中$g_j$表示从$Gumbel(0,1)$分布中获得的噪声样本，$\tau$表示温度系数，$\tau$较大时
噪声的权重较大，编码配置的选择过程接近随机采样，$\tau$越小，编码配置选择过程的随机性也越小。

这种方法在优化初期引入的噪声可能会造成优化过程不稳定的问题，
因此与DeepSeekMoE\cite{dai2024deepseekmoe}将混合专家模型中的专家划分为共享专家与路由专家两部分类似，
将选择编码配置的过程划分为选择稳定配置$\mathcal{I}_s$与选择随机配置$\mathcal{I}_r$两部分：
\begin{equation}
\label{eq2}
    \mathcal{I}_s=\left\{i|s_i\in\text{Topk}(\{s_i|j\in\mathcal{I}\},N_s)\right\}
\end{equation}
\begin{equation}
\label{eq3}
\mathcal{I}_r=\left\{i|s_i+\tau*g_j\in\text{Topk}(\{s_i+\tau*g_j|j\in\mathcal{I}-\mathcal{I}_s\},N_r)\right\}
\end{equation}
首先利用\eqref{eq2}计算出$N_s$个稳定配置$\mathcal{I}_s$，然后利用\eqref{eq3}在剩余配置中计算出$N_r$个随机配置$\mathcal{I}_r$。
最终选中的编码配置集合$\mathcal{I}^*$为：
\begin{equation}
\mathcal{I}^*=\mathcal{I}_{s}\cup\mathcal{I}_r
\end{equation}
接下来使用上一章提出的编码配置选择器选择出最佳配置。

设输入纹理块 $\mathbf{B}\in\mathbb{R}^{b\times n}$，其中 $b$ 是一个纹理块中的纹素个数。
基于混合专家模型选中的编码配置集合为 $\mathcal{I}^*$，$\mathbf{B}$使用配置 $i\in\mathcal{I}^*$ 
经过可微编解码后得到的纹理块为 $\mathbf{B}'_i\in\mathbb{R}^{b\times n}$，
对应的压缩误差为 $L_i$。
\begin{equation}
L_i=\|\mathbf{B}-\mathbf{B}'_i\|_F^2
\end{equation}
\begin{equation}
\mathbf{B}'_{i^*}=\sum_{i\in\mathcal{I}} w_i\mathbf{B}'_i
\end{equation}
\begin{equation}
    w_i =\left\{\begin{matrix}
        1,& i=\mathop{\arg\min}\limits_{i} L_i
        \\0,& i\ne\mathop{\arg\min}\limits_{i} L_i
        \end{matrix}\right.
\end{equation}
其中 $w_i$ 是配置 $i$ 的组合系数。由于最终只选择使得$L_i$最小的配置，
因此实际上 $w_i$ 是 one-hot 的。
设所有编码配置集合$\mathcal{I}$中的配置数为$N$，
这个方法的计算量是原来的$\frac{N_s+N_r}{N}$，当$N_s+N_r=N$时等价于上一章中的方法。

该方法的整体框架如图\ref{fig:Mop}所示，
由于BC6格式分区配置占配置总数的33/34，BC7格式分区配置占配置总数的
64/77，因此实现时只对分区配置使用基于MoE的编码配置选择器，
对于数量较少的其他配置会视为激活的稳定配置。
使用MoE配置选择器的DBC模型称为DBCM，
其优点包括：
\begin{itemize}
\item 稳定性：稳定编码配置保证总是会计算亲和度较高的编码配置，使得训练更稳定。
\item 探索性：随机编码配置保证总是会探索剩余的编码配置，因为即使有些配置在某次优化迭代中亲和度不高，
但随着训练的进行，这些配置的亲和度可能会变高。
\item 效率：通过结合稳定编码配置与随机编码配置的选择策略，
该模型在确保训练稳定性和探索性的同时显著提高了计算效率。
\end{itemize}
\begin{figure}[htbp]
    \centering
    \includegraphics[width=1\textwidth]{figures/Mop_v2.pdf}
    \caption{基于混合专家模型的编码配置选择器}
    \label{fig:Mop}
\end{figure}


\section{实验设计与分析}

本章将基于混合专家模型的编码配置选择器加入
DBC模型，称为DBCM模型，并在神经压缩模型、
基于移动基分解的光照数据压缩模型
\cite{silvennoinen2021moving}（Moving Basis Decomposition，MBD）和
HDR 纹理的 RGBM 编码模型，三个应用场景中进行神经纹理的压缩实验。
神经压缩模型中，神经纹理作为神经特征进行训练。
MBD模型中，神经纹理作为插值基向量的系数进行训练。
RGBM编码模型中，神经纹理作为HDR纹理经过RGBM编码产生的LDR纹理进行训练。
实验过程中使用i7-13700KF和RTX 2080Ti进行测试。

\subsection{实验设置}

\paragraph{实验数据集}

在神经压缩模型的实验中，使用来自polyhaven.com\cite{PolyHaven}的 6 个公开的材质纹理集，
每个纹理集包含 8 个通道，由 3 张纹理组成：3通道的漫反射问题、2通道的法线纹理 和 3通道的ARM纹理（环境光遮蔽、粗糙度、金属度），
分辨率均为2048×2048(2K)。

在MBD的实验中，使用UE引擎烘焙的场景光照数据，该数据包含 5 个场景，
分别命名为 Cornell Box、Scene 1、Scene 2、Scene 3 和 Scene 4。

在RGBM的实验中，使用来自polyhaven.com\cite{PolyHaven}的 6 个公开的HDR图像，
分辨率均为1024×1024(1K)。

\paragraph{标准DXT格式编解码器}

本章也使用英伟达开发的纹理压缩工具NVTT(NVIDIA Texture Tools)，作为标准DXT格式编解码器。
接入DBC模型的实验模型优化结束后，
需要使用NVTT对神经纹理进行DXT格式压缩，
以导出DXT格式进行测试，从而验证DBC模型的有效性。

\paragraph{评估指标}

神经压缩模型与RGBM编码模型的实验使用
峰值信噪比(Peak Signal-to-Noise Ratio, PSNR)、
结构相似性\cite{wang2004image}(Structural Similarity, SSIM)
以及FLIP\cite{andersson2020flip}进行评估。
MBD模型的实验对于光照数据本身的压缩损失使用均方根误差
（Root Mean Squared Error，RMSE）进行评估，对于使用光照数据
渲染的场景图像使用PSNR与FLIP进行评估。


\paragraph{对比方法}

本章改进的DBC模型称为DBCM。
神经压缩实验中使用以下方法进行对比：

\begin{itemize}
    \item BC6，BC7： 训练过程不使用DBC模型。
    \item DBC6s，DBC7s：： 训练过程使用DBC模型，但不使用配置选择器，以固定配置进行训练，
    相当于仅解码器的可微BC6/BC7。
    \item DBC6，DBC7： 训练过程使用上一章提出的带有配置选择器的DBC模型。    
\end{itemize}

这些方法在实验测试时都会使用NVTT将神经纹理导出为BC6/BC7格式进行测试。
MBD光照数据压缩实验与RGBM编码实验由于神经纹理为RGBA格式，因此
使用DBC7M模型进行实验，实验仅验证DBCM模型的有效性，因此这两个实验中
DBCM模型仅与直接压缩对比。

\paragraph{实现细节}

DBCM模型中的$N_s$与$N_r$都设为2，以兼顾计算效率与编码质量。
神经压缩实验与RBM编码实验的学习率都初始化为0.01，
MBD实验的学习率初始化为0.1。训练过程都使用Adam优化器。
对于神经压缩实验，具体配置与上一章相同，会进行10000次迭代的预训练。
对于MBD实验，进行5000次迭代的预训练。
对于RGBM编码实验，由于RGBM模型本身无需训练，
因此不进行预训练，将HDR纹理通过RGBM编码为LDR纹理作为初始化的神经纹理。

三个实验的训练过程都使用上一章中的学习率衰减策略以及停止策略，
优化完成后使用NVTT将神经纹理导出为BC6/BC7格式进行测试。

\subsection{实验结果与分析}

\subsubsection{对比实验}
\label{4:对比实验}

神经压缩实验的实验结果如图表\ref{6个材质的纹理集上进行的对比实验}所示，
相较于上一章的DBC模型，改进的DBCM模型在大部分实验中接近DBC模型，
由于随机配置的加入，在一些实验用例中甚至超过DBC模型。
实验结果证明了DBCM模型的有效性。

\begin{table*}[htbp]
    \centering
    \caption{6个材质的纹理集上进行的对比实验}
    \label{6个材质的纹理集上进行的对比实验}        
    \resizebox{\linewidth}{!}{
    \begin{tabular}{ccccccccccccccccccc}
        \toprule
        & \multicolumn{3}{c}{Ukulele\_01} & \multicolumn{3}{c}{antique\_katana\_01} & \multicolumn{3}{c}{utility\_box\_02} & \multicolumn{3}{c}{chinese\_chandelier} & \multicolumn{3}{c}{dartboard} & \multicolumn{3}{c}{Drill\_01}\\
                  & PSNR(↑)  & SSIM(↑) & FLIP(↓)& PSNR(↑) & SSIM(↑) & FLIP(↓)& PSNR(↑) & SSIM(↑) & FLIP(↓) & PSNR(↑) & SSIM(↑)& FLIP(↓) & PSNR(↑)& SSIM(↑)& FLIP(↓)& PSNR(↑) & SSIM(↑)& FLIP(↓)\\
        \midrule
        BC6       & 22.3005  & 0.6716  & 0.2713 & 24.1197 & 0.7007  & 0.2161 & 23.8151 & 0.6977  & 0.2341  & 20.1739 & 0.604  & 0.3457 & 20.4314 & 0.5262 & 0.3073 & 22.6281 & 0.7039 & 0.2976  \\
        DBC6s     & 27.1085  & 0.7625  & 0.1705 & 27.8752 & 0.8197  & 0.1332 & 28.027  & 0.7853  & 0.1472  & 25.7551 & 0.7407 & 0.2197 & 23.7457 & 0.5955 & 0.2289 & 26.8157 & 0.8328 & 0.163  \\
        DBC6      & 26.9937  & 0.7613  & 0.1726 & 27.8596 & 0.8205  & 0.1346 & 28.1131 & 0.7893  & 0.1446  & 25.7037 & 0.7446 & 0.218  & 23.8662 & 0.5981 & 0.2254 & 26.8698 & 0.831  & 0.1665   \\
        DBC6M     & 28.271   & 0.7828  & 0.1464 & 28.9463 & 0.8432  & 0.1134 & 28.7504 & 0.8018  & 0.1326  & 26.8738 & 0.7626 & 0.1942 & 24.7159 & 0.6135 & 0.2078 & 28.2355 & 0.8549 & 0.1394  \\
        \midrule
        BC7       & 23.0234  & 0.6685  & 0.2606 & 25.0525 & 0.6602  & 0.2262 & 25.0392 & 0.7163  & 0.2338  & 22.45   & 0.6358 & 0.3009 & 21.5528 & 0.5536 & 0.3052 & 22.2064 & 0.7357 & 0.2971   \\
        DBC7s     & 30.2512  & 0.8221  & 0.1087 & 30.602  & 0.8683  & 0.0919 & 30.8006 & 0.8338  & 0.1068  & 29.6132 & 0.8223 & 0.1378 & 26.6591 & 0.6551 & 0.1671 & 31.8172 & 0.9084 & 0.0986  \\
        DBC7      & 30.7054  & 0.8328  & 0.0953 & 31.5834 & 0.8921  & 0.0748 & 31.1972 & 0.8431  & 0.1007  & 30.0198 & 0.8346 & 0.1269 & 27.2124 & 0.6682 & 0.1584 & 33.8654 & 0.9371 & 0.0718   \\
        DBC7M     & 30.633   & 0.8304  & 0.0966 & 31.4472 & 0.8917  & 0.0761 & 31.2531 & 0.844   & 0.096   & 30.0176 & 0.8314 & 0.1273 & 27.2132 & 0.6679 & 0.1579 & 33.6313 & 0.9326 & 0.0752  \\
        \bottomrule
\end{tabular} }   
\end{table*}

MBD实验的结果如表\ref{MBDexp1}所示，
该表列出了5个场景中不同方法的RMSE和存储大小的具体值。
可以看出，MBD+DBCM的RMSE明显低于MBD+BC7，
降低幅度为60\%至80\%，同时占用的存储空间大小仅为MBD方法的1/4。

\begin{table*}[htbp]
    \centering
    \caption{5个烘焙的场景光照数据进行的对比实验}
    \label{MBDexp1}        
    \resizebox{\linewidth}{!}{
    \begin{tabular}{lllllllllll}
        \toprule
                    & \multicolumn{2}{l}{Cornell Box} & \multicolumn{2}{l}{Scene 0} & \multicolumn{2}{l}{Scene 1} & \multicolumn{2}{l}{Scene 2} & \multicolumn{2}{l}{Scene 3} \\
                    & RMSE       & Memory Size / KB   & RMSE     & Memory Size / KB & RMSE     & Memory Size / MB & RMSE     & Memory Size / MB & RMSE     & Memory Size / MB \\
        \midrule
    MBD + BC7       & 0.427069   & 4                  & 0.312421 & 96.25            & 0.462323 & 0.29             & 0.141934 & 0.38             & 0.415555 & 1.24             \\
    MBD + DBC7M        & 0.183869   & 4                  & 0.150063 & 96.25            & 0.292717 & 0.29             & 0.114637 & 0.38             & 0.376664 & 1.24             \\
    MBD             & 0.078967   & 16                 & 0.042723 & 385              & 0.071571 & 1.15             & 0.065373 & 1.53             & 0.198106 & 4.96             \\
    Reference       & -          & 432                & -        & 4,620            & -        & 14.13            & -        & 18.47            & -        & 59.55           \\
\bottomrule
\end{tabular} }   
\end{table*}

MBD实验中对两个场景的光照数据进行了可视化，如图\ref{fig:MBD_v3}所示
MBD+BC7的结果显示出明显的块状，
而MBD+DBC7M结果中的块状不明显。
这表明加入DBCM模型的端到端优化有效减少了BC7造成的损失。
MBD+DBC7M的结果的视觉质量与MBD的结果没有太大不同，而存储大小仅为1/4。

\begin{figure*}[htbp]
    \centering
    \includegraphics[width=1.0\linewidth]{figures/MBD_v3.pdf}
    \caption{两个场景光照数据的可视化实验}
    \label{fig:MBD_v3} 
\end{figure*}

\begin{table*}[htbp]
    % \begin{minipage}{\columnwidth}
    \caption{6个HDR图像上进行的对比实验}
    \label{tb:RGBM}
    \begin{center}
    \begin{tabular}{ccccccccccccccccccc}
    \toprule
                    & \multicolumn{3}{c}{RGBM+BC7}  & \multicolumn{3}{c}{RGBM+DBC7M}  \\ 
                    & PSNR(↑)   & SSIM(↑) & FLIP(↓)&PSNR(↑)& SSIM(↑) & FLIP(↓) \\
    \midrule
lakeside\_sunrise    & 37.4817   & 0.9666 & 0.0679 &44.1479&0.986&0.0607 \\
mirrored\_hall       & 35.3975   & 0.9148 & 0.0814 &40.5526&0.9832&0.0539 \\
stierberg\_sunrise   & 34.8078   & 0.9457 & 0.0621 &42.5311&0.9868&0.0433 \\
studio\_garden       & 35.7492   & 0.9561 & 0.1014 &44.8351&0.9918&0.0909 \\
urban\_street        & 38.7993   & 0.9748 & 0.0586 &47.4841&0.992&0.0548 \\
vignaioli\_night     & 38.3124   & 0.9814 & 0.0721 &43.5846&0.9942&0.0607 \\
    \bottomrule
    \end{tabular}
    \end{center}
    % \end{minipage}
\end{table*}

\begin{figure*}[htbp]
    \centering
    \includegraphics[width=1.0\linewidth]{figures/RGBM.pdf}
    \caption{两个HDR纹理的可视化实验}
    \label{fig:RGBM} 
\end{figure*}

RGBM编码的实验结果如表\ref{tb:RGBM}所示，
DBCM模型有效降低了BC7压缩造成的误差。
对vignaioli\_night与studio\_garden进行了可视化，结果如图\ref{fig:RGBM}所示，
RGBM+BC7方法明显造成了图块模糊，而RGBM+DBC7M方法视觉效果上接近参考图像。


\subsubsection{性能测试}

在神经压缩实验6个材质的纹理集上进行了不同方法的性能测试，
其中DBCM模型的$N_s$与$N_r$都为2。实验结果取6次实验的平均值，
如表\ref{4:6个材质的纹理集上进行的性能实验}所示。
DBCM模型由于使用了较少的编码配置数，相比上一章的DBC模型
运行速度显著提高，其中DBC7M与DBC7相比速度提升了一个数量级。

\begin{table*}[htbp]
    \centering
    \caption{6个材质的纹理集上进行的性能实验}
    \label{4:6个材质的纹理集上进行的性能实验}        
    \resizebox{0.75\linewidth}{!}{
    \begin{tabular}{ccccccccccccccccccc}
        \toprule
                     & BC6 & DBC6s & DBC6 &DBC6M& BC7  & DBC7s & DBC7 &DBC7M\\
        \midrule
        运行时间(ms)  & 33  &  80   & 1012 & 166   & 35   & 171 & 2938 &  278 \\
        \bottomrule
\end{tabular} } 
\end{table*}

\subsubsection{参数敏感性实验}

DBCM模型中的编码配置选择器具有$N_s$与$N_r$
两个参数，为了验证改进的编码配置选择器效果，
使用不同的$N_s$与$N_r$组合在神经压缩模型中进行了实验，
实验使用 6 个材质纹理集中的chinese\_chandelier。
除$N_s$与$N_r$外，其余实验设置与之前的神经压缩实验相同。
实验结果如表\ref{tb:NsNr}所示，
其中最右边的$N_s=64$， $N_r=0$相当于上一章的DBC模型。
实验结果中$N_s=N_r$时产生了最佳性能，
实现了与穷举编码配置的DBC模型相当的编码质量，
同时相对固定配置仅增加了少量计算开销。

\begin{table*}[htbp]
    \centering
    \caption{ $N_s$与$N_r$的参数敏感性实验结果}
    \label{tb:NsNr}
    \resizebox{\linewidth}{!}{
    \begin{tabular}{ccccccccccccccccccc}
    \toprule
    $N_s$ / $N_r$ & 0 / 2 & 1 / 1 & 2 / 0 & 0 / 4 & 2 / 2 & 4 / 0 & 0 / 8 & 4 / 4 & 8 / 0 & 64 / 0 \\
    \midrule
    PSNR (dB)     & 29.5630 & \textbf{29.9851} & 29.8575 & 29.7333 & \textbf{30.0586} & 29.9351 & 29.8061 & \textbf{30.0223} & 30.0067 & 30.0684 \\
    SSIM          & 0.8708  & \textbf{0.8737}  & 0.8701  & 0.8721  & \textbf{0.8740}  & 0.8719  & 0.8739  & \textbf{0.8741}  & 0.8740  & 0.8752  \\
    FLIP          & 0.0937  & \textbf{0.0908}  & 0.0933  & 0.0922  & \textbf{0.0903}  & 0.0922  & 0.0925  & \textbf{0.0900}  & 0.0901  & 0.0899  \\
    \bottomrule
    \end{tabular}}
\end{table*}

\section{本章小结}

本章提出了一种基于MoE的编码配置选择器，
相比固定配置的DBC模型，仅增加少量计算开销就能
接近穷举所有编码配置的效果。
引言部分提出了DBC模型穷举所有编码配置
造成大量计算开销的问题。
然后分析了混合专家模型与编码配置选择器的相似性，并
提出改进的编码配置选择器的整体框架。
最后在神经压缩、MBD光照数据压缩、RGBM编码
三个应用场景中对本章所提出的方法进行测试，
以证明基于DBC模型改进的DBCM模型的有效性。