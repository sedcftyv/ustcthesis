% !TeX root = ../main.tex

\chapter{基于混合专家模型的分区配置选择器}

\section{引言}

上一章提出的可微块压缩模型中，为了在多种编码配置中选择最佳配置，
采用了与标准DXT格式编解码器类似的方法，即在可微编解码过程中尝试所有编码配置，
并根据每种编码配置的压缩误差选择最佳编码配置。这种方法简单并且易于实现，
但会造成显著的计算开销。
例如上一章中的DBC6在每次迭代过程进行可微编解码时，需要计算32种双分区$+$1种单分区共33种编码配置，
DBC7需要计算64种双分区$+$1种单分区$+$4种旋转\times3种不同量化方案共77中编码配置，
相较于仅计算一种编码配置的固定编码配置的DBC模型，显著增加了运行时间。
分区配置在BC6/BC7中的所有编码配置中占比最高（BC6格式分区配置占配置总数的33/34，BC7格式分区配置占配置总数的
64/77），因此优化大量分区配置的计算即可显著提高编码配置选择过程的计算效率。
为了优化大量分区配置造成的计算开销，
本章提出了一种基于混合专家模型的分区配置选择器，
这个模块的作用是从大量分区配置中启发式地选择出
优先级较高的分区配置，通过只使用优先级较高的
分区配置进行编解码，
在几乎不影响编码质量的前提下，
提高可微块压缩模型(DBC)的计算效率。

\section{问题描述与分析}

对于DXT格式中的多模式格式（例如BC6与BC7），对于纹理中的每4x4纹理块编码时都会选择
其中一种编码配置进行编码（例如不同的颜色端点与权重量化位数、
某种特定的分区操作或将RGB通道之一与A通道交换的旋转操作等）。
在标准DXT格式编解码器中，为了确定最佳的编码配置，通常会使用多种编码配置进行编解码，并选择使得压缩误差最低的
编码配置作为最优配置。对于面向压缩任务的标准编解码器，
这个过程对于某个纹理块只进行一次，因此带来的计算开销可以被接受。
但对于本文提出的面向优化任务的可微编解码器，这个过程会在优化过程中每次迭代进行一次，
因此带来的计算开销非常显著，如上一章中的\ref{3:性能测试}节所示。
而分区配置在BC6/BC7中的所有编码配置中占绝大部分（BC6格式分区配置占配置总数的33/34，BC7格式分区配置占配置总数的
64/77），因此优化分区配置的选择可以减少编码配置动态选择过程中主要的计算开销来源。

为解决这一问题，首先需要分析上一章中基于穷举法的编码配置选择过程。
设输入纹理块 $\mathbf{B}\in\mathbb{R}^{b\times n}$，其中 $b$ 是一个纹理块中的纹素个数。
所有配置的集合为 $\mathcal{I}$，针对配置 $i\in\mathcal{I}$ 经过可微编解码后得到的压缩纹理块为 $\mathbf{B}'_i\in\mathbb{R}^{b\times n}$。
定义编码配置$i$对应的压缩误差 $L_i$ 为
\begin{equation}
L_i=\|\mathbf{B}-\mathbf{B}'_i\|_F^2
\end{equation}
对每4x4纹理块$\mathbf{B}$，配置选择器选择使得压缩误差$L_i$最小的编码配置$i^*$：
\begin{equation}
    \label{4:argmin}
    i^*=\mathop{\arg\min}\limits_{i} L_i
\end{equation}
为了确定每种编码配置$i$对应的编码误差$L_i$，
上一章中使用了穷举的方法，对每4x4纹理块都会使用所有编码配置进行编码，
从而计算每4x4纹理块所有编码配置对应的编码误差。
由于优化过程需要进行大量迭代，因此对编码配置的穷举造成了显著的计算开销。

根据公式\ref{4:argmin}可知，即使计算出了每种编码配置$i$对应的编码误差$L_i$，
最终有效的编码配置依然只有其中一个，因此大部分的计算对最终结果都没有
任何贡献。如果在不计算每种编码配置的编码误差的情况下，依旧能选择出
最佳的编码配置，就可以大量节省计算开销。

一种启发式的想法是，迭代过程开始前初始化一个编码误差表，其中存储
每4x4块中每种编码配置对应的编码误差。
每次迭代中根据编码误差表选择一些编码误差较小的编码配置进行计算，
同时迭代过程始终保持更新编码误差表。
相当于使用历史迭代的编码误差估计当前迭代中的编码误差。

这种方法每次迭代只需计算少量编码配置，因此可以显著节省计算开销。
但与之而来的问题是该方法只更新每次迭代中被选中（激活）的少量编码配置，
可能导致优化过程中每个块的编码配置趋于固定，导致DBC模型退化为
上一章中固定编码配置的DBCs模型。

可以使用强化学习中智能体的探索策略，每次迭代中
除了选择编码误差较小的编码配置外，对于剩余的编码配置
按照对应的编码误差进行随机采样，使编码误差暂时较大的编码配置
也有被选中的可能，防止频繁地只选中少量编码配置，导致方法退化。

因此本章提出了一种基于混合专家模型的分区配置选择器，每次迭代过程中
根据编码误差表将分区配置分为稳定分区配置与随机分区配置，
从而兼顾对编码配置表的探索与利用，在保持较高编码质量的前提下，
减少分区配置选择造成的大量计算开销。

\section{模型框架}

最初的混合专家模型\inlinecite{jacobs1991adaptive}中，每个专家都代表一个前馈神经网络，
还有一个门控网络用于选择使用哪个专家的结果作为输出。专家网络和门控网络都接收相同的输入，
门控网络将输入乘上一个可训练的权重矩阵后，使用softmax层获得每个专家的概率分布，
最后根据这个概率分布在所有专家中随机采样其中一个并作为输出。
最近的DeepSeekMoE\cite{dai2024deepseekmoe}将一部分专家隔离为共享专家，其他专家被称为路由专家。
共享专家的输出总是有效。路由专家通过门控网络控制，门控网络的输出被称为亲和度(affinity)，
亲和度最高的前k个路由专家的输出有效，然后通过亲和度对有效的路由专家的输出进行加权混合。
最终的输出为将共享专家与加权后的有效路由专家两部分输出之和。

纹理编解码过程中一种分区编码配置可以视作一个专家。
对于分区编码配置的选择基于编码误差，因此可以使用分区配置的编码误差表替换门控网络
用于进行专家选择。为了减少大量分区配置造成的计算开销，
每次迭代过程中从分区配置的编码误差表中
只选择一部分分区配置作为激活配置，
与其余非分区配置一起进行可微编解码并计算编码误差，
同时更新分区配置的编码误差表，
最后通过配置选择器根据编码误差输出最终的可微编解码结果。

由于每次计算过程中实际只计算并更新了部分分区配置的编码误差，
为了防止退化为固定配置的DBCs模型，
因此除了利用编码误差表选出
优先级更高的分区配置外，还需要基于编码误差表进行一定随机性的探索，
从而使得编码误差表中编码误差暂时较大的分区配置也机会得到使用。

因此与DeepSeekMoE\cite{dai2024deepseekmoe}将混合专家模型中的专家
分为共享专家与路由专家两部分类似，本章将分区配置分为
稳定分区配置与随机分区配置。每次迭代过程中，利用编码误差表
激活编码误差较小的分区配置作为稳定分区配置，在剩余分区配置的编码误差表中将编码误差作为概率，
随机采样出随机分区配置，两部分组合成为本次迭代中激活的分区配置。
为了实现根据编码误差的随机采样，使用了强化学习中常用的Gumbel-Max技巧\cite{jang2016categorical}。
Gumbel-Max技巧提供了一种简单而有效的方法，从分类概率为$\pi$的
类别分布中采样样本$z$：
\begin{equation}
z = \arg\max_i \left( \log\pi_i + g_i \right)
\end{equation}
其中$g_i$是从$Gumbel(0, 1)$分布中采样的噪声：
\begin{equation}
    g = -log(-log(u))
\end{equation}
其中$u$表示均匀分布$Uniform(0, 1)$分布中采样的随机数，
$g$表示采样得到的Gumbel噪声。

综上所述，本章提出了一种基于混合专家模型的分区配置选择器，
称为MoP(Mixture of Partitions)。
MoP的具体过程如图\ref{fig:Mop}所示，其中Affinity Table为
分区配置的亲和度表，值为编码误差的倒数，亲和度越大则相应的编码误差越小。
每次迭代时，根据分区配置的亲和度表进行Top-K排序，
选择前$N_s$大的亲和度对应的编码配置作为稳定分区配置，
对于亲和度表中剩余分区配置通过Gumbel-Max技巧采样$N_r$个随机配置。
最终一共$N_s+N_r$个分区配置被激活，并进行可微编解码。
对于非分区配置，由于数量较少，因此每次迭代时都会进行计算，
相当于将非分区配置都作为稳定配置。当$N_s+N_r$等于分区配置总数时，
MoP的计算开销等于上一章中使用穷举法的DBC模型。

\begin{figure*}[htbp]
    \centering
    \includegraphics[width=0.7\textwidth]{figures/Mop_v3.pdf}
    \caption{基于混合专家模型的分区配置选择器(MoP)}
    \label{fig:Mop}
\end{figure*}

\begin{figure*}[htbp]
    \centering
    \includegraphics[width=1\textwidth]{figures/DBC_overview_v4.pdf}
    \caption{基于MoP的DBCM模型的整体框架}
    \label{fig:DBCM}
\end{figure*}

为了与上一章提出的DBC模型进行区分，本章提出的具有分区配置选择器
的可微块压缩模型称为DBCM模型。DBCM模型的整体框架如图\ref{fig:DBCM}所示。
神经纹理的每4x4块的编码配置分为分区配置与非分区配置，
图中的MoP Configs表示通过MoP激活的分区配置，未激活的分区配置
则在此次迭代中不进行计算。神经纹理每4x4的块经过MoP激活的分区配置
与非分区配置进行可微编解码后，使用配置选择器根据编码误差选择输出时，
对于编码误差也使用了Gumbel-Max技巧，因此将DBCM的配置选择器
称为Gumbel配置选择器。与之前MoP中的Gumbel-Max技巧不同，
Gumbel配置选择器将Gumbel噪声乘温度项$\tau$，
随着温度的降低，Gumbel噪声逐渐减弱，Gumbel配置选择器
逐渐转变为上一章中的argmax配置选择器。
这样做的原因是神经纹理在优化过程中不断发生变化，
在优化早期使用的编码配置可能对优化后期的同一个纹理块
不再适用，因此类似模拟退火算法，在优化初期进行一定程度的探索，
从而更好地搜索到每个块最佳的编码配置。

\section{实现细节}

DeepSeekMoE\cite{dai2024deepseekmoe}中的
门控网络输出路由专家对输入的亲和度，并根据亲和度选择路由专家，
而纹理编解码过程只需要选择编码误差最小的编码配置，因此可以使用编码误差的倒数作为对分区配置的亲和度，
同时无需使用门控网络：
\begin{equation}
s_i=\frac{1}{L_i}
\end{equation}
其中$s_i$表示分区配置$i$的亲和度，
$L_i$表示使用分区配置$i$产生的压缩误差。

为了减少大量分区配置造成的计算开销，每次迭代时仅激活部分的分区配置，
因此分区配置的亲和度表中，激活的分区配置可以得到更新，
未激活的分区配置使用之前迭代中计算出的历史亲和度近似。
假设某次迭代选中的分区配置集合为$\{i|i\in\mathcal{I}^*\}$，更新亲和度表时：
\begin{equation}
    s_i\leftarrow
    \begin{cases} 
        (1-\alpha)s_i+\alpha\frac{1}{\|B-B'_i\|_F^2}, & \text{if } i\in\mathcal{I}^*,\\
    s_i, & \text{otherwise}.
    \end{cases}
\end{equation}
其中$B$表示可微编解码前的4x4纹理块，$B'_i$表示$B$使用分区配置$i$可微编解码后的4x4纹理块。
$\alpha$ 是滑动平均系数，固定为0.2，$\alpha$的作用是使得亲和度表
的更新更加平缓，防止随机分区配置具有的随机性导致亲和度表中的值变化过大，造成训练过程不稳定。

优化前需要对分区配置的亲和度表$\{s_i|i\in\mathcal{I}\}$进行初始化，
初始化方法为使用穷举法计算一次每4x4块中所有分区配置对应的编码误差的倒数，将其作为
分区配置的亲和度。

亲和度表存储了每4x4纹理块中每个分区配置对应的亲和度。
基于亲和度表可以通过Top-K方法选出优先级较高的稳定分区配置。
具体来说，首先根据亲和度表从所有分区配置集 $\mathcal{I}$ 中
选出亲和度最大的前$N_s$个分区配置作为稳定分区配置$\mathcal{I}_s$ ，即
\begin{equation}
\label{eq41}
\mathcal{I}_s=\left\{i|s_i\in\text{Topk}(\{s_i|j\in\mathcal{I}\},N_s)\right\}
\end{equation}
其中 $\text{Topk}(\mathcal{S},n)$ 表示从集合 $\mathcal{S}$ 中选出最大的 $n$ 个元素组成的集合，
$s_i$表示亲和度表中分区配置$i$的亲和度。

为了防止总是使用数量较少的稳定分区配置导致方法退化为固定配置。
与\inlinecite{shazeer2017outrageously}中的Noisy Top-K门控类似，
在对亲和度表进行Top-K计算时，通过增加噪声的方式，实现编码配置的随机采样，
从而尝试更多的分区配置。
因此使用强化学习中常用的Gumbel-Max技巧\cite{jang2016categorical}向\eqref{eq41}中添加了Gumbel噪声\cite{jang2016categorical}
实现将亲和度作为概率采样$N_r$个随机分区配置$\mathcal{I}_r$：
\begin{equation}
\mathcal{I}_r=\left\{i|s_i+g_j\in\text{Topk}(\{s_i+g_j|j\in\mathcal{I}-\mathcal{I}_s\},N_r)\right\}
\end{equation}
其中 $\text{Topk}(\mathcal{S},n)$ 表示从集合 $\mathcal{S}$ 中选出最大的 $n$ 个元素组成的集合
，$g_j$表示从$Gumbel(0,1)$分布中获得的噪声样本。

最终激活的分区配置集合$\mathcal{I}^*$为：
\begin{equation}
\mathcal{I}^*=\mathcal{I}_{s}\cup\mathcal{I}_r
\end{equation}
接下来激活的分区配置与非分区配置一起作为激活的编码配置，
通过可微编解码器对4x4纹理块进行编解码。
最后使用Gumbel配置选择器选择出最佳配置：

设输入纹理块 $\mathbf{B}\in\mathbb{R}^{b\times n}$，其中 $b$ 是一个纹理块中的纹素个数。
基于混合专家模型选中的编码配置集合为 $\mathcal{I}^*$，$\mathbf{B}$使用配置 $i\in\mathcal{I}^*$ 
经过可微编解码后得到的纹理块为 $\mathbf{B}'_i\in\mathbb{R}^{b\times n}$，
对应的压缩误差为 $L_i$。
\begin{equation}
L_i=\|\mathbf{B}-\mathbf{B}'_i\|_F^2
\end{equation}
\begin{equation}
\mathbf{B}'_{i^*}=\sum_{i\in\mathcal{I}} w_i\mathbf{B}'_i
\end{equation}
\begin{equation}
    w_i =\left\{\begin{matrix}
        1,& i=\mathop{\arg\min}\limits_{i} (L_i+\tau*g_j)
        \\0,& i\ne\mathop{\arg\min}\limits_{i} (L_i+\tau*g_j)
        \end{matrix}\right.
\end{equation}
其中 $w_i$ 是配置 $i$ 的组合系数，由于最终只选择使得$L_i+\tau*g_j$最小的配置，
因此实际上 $w_i$ 是 one-hot 的，
$g_j$表示从$Gumbel(0,1)$分布中获得的噪声样本，
$\tau$表示温度系数，$\tau$较大时
噪声的权重较大，编码配置的选择过程接近随机采样，$\tau$越小，编码配置选择过程的随机性也越小。
当$\tau=0$时，Gumbel配置选择器等价于上一章中的配置选择器。
每次迭代过程中使用指数衰减的方式，令$\tau$不断减小，从而保证优化初期对编码配置的探索，
优化后期以编码误差最小的方式选择编码配置。

设所有分区配置集合$\mathcal{I}$中的配置数为$N$，
DBCM模型的计算量是原来的$\frac{N_s+N_r}{N}$，当$N_s+N_r=N$时计算量等于上一章中的DBC模型。
DBCM的优点包括：
\begin{itemize}
\item 稳定性：稳定分区配置保证总是会激活分区配置中编码误差较小的分区，
随机分区配置保证优化过程不会退化为固定分区配置。
这种方式与标准DXT格式编解码器相同，保证可微近似的正确性，同时使得训练过程比较稳定。
\item 探索性：Gumbel配置选择器保证对编码配置的探索，因为优化过程的神经纹理不断发生变化，
优化初期的编码配置不一定适合优化后期的神经纹理，因此在优化初期进行探索，优化后期逐渐固定到某一个配置的
模拟退火过程更容易找到全局最优的编码配置。
\item 效率：通过结合稳定分区配置与随机分区配置的选择策略，该模型在确保训练稳定性和探索性的同时可以显著提高计算效率。
\end{itemize}


\section{实验设计与分析}

本章将基于混合专家模型的分区配置选择器(MoP)加入
DBC模型，称为DBCM模型，并在神经压缩模型、
基于移动基分解的光照数据压缩模型
\cite{silvennoinen2021moving}（Moving Basis Decomposition，MBD）和
HDR 纹理的 RGBM 编码模型，三个应用场景中进行神经纹理的压缩实验。
神经压缩模型中，神经纹理作为神经特征进行训练。
MBD模型中，神经纹理作为插值基向量的系数进行训练。
RGBM编码模型中，神经纹理作为HDR纹理经过RGBM编码产生的LDR纹理进行训练。
实验过程中使用i7-13700KF和RTX 2080Ti进行测试。

\subsection{实验设置}

\paragraph{实验数据集}

在神经压缩模型的实验中，使用来自polyhaven.com\cite{PolyHaven}的 6 个公开的材质纹理集，
每个纹理集包含 8 个通道，由 3 张纹理组成：3通道的漫反射问题、2通道的法线纹理 和 3通道的ARM纹理（环境光遮蔽、粗糙度、金属度），
分辨率均为2048×2048(2K)。

在MBD的实验中，使用UE引擎烘焙的场景光照数据，该数据来自 5 个场景，
分别命名为 Cornell Box、Scene 1、Scene 2、Scene 3 和 Scene 4。
其中Cornell Box场景的光照数据使用二阶球谐函数存储，其他4个场景的光照数据
使用一阶球谐函数存储。

在RGBM的实验中，使用来自polyhaven.com\cite{PolyHaven}的 6 个公开的HDR图像，
分辨率均为1024×1024(1K)。

\paragraph{标准DXT格式编解码器}

本章也使用英伟达开发的纹理压缩工具NVTT(NVIDIA Texture Tools)，作为标准DXT格式编解码器。
接入DBCM模型的神经纹理优化结束后，
需要使用NVTT对神经纹理进行DXT格式压缩，
以导出DXT格式进行测试，从而验证DBCM模型的有效性。

\paragraph{评估指标}

神经压缩模型与RGBM编码模型的实验使用
峰值信噪比(Peak Signal-to-Noise Ratio, PSNR)、
结构相似性\cite{wang2004image}(Structural Similarity, SSIM)
以及FLIP\cite{andersson2020flip}进行评估。
MBD模型的实验对于光照数据本身的压缩损失使用均方根误差
（Root Mean Squared Error，RMSE）进行评估，对于使用光照数据
渲染的场景图像使用PSNR与FLIP进行评估。


\paragraph{对比方法}

本章改进的DBC模型称为DBCM。
神经压缩实验中使用以下方法进行对比：

\begin{itemize}
    \item BC6，BC7： 训练过程不使用DBC模型。
    \item DBC6s，DBC7s：： 训练过程使用DBC模型，但以固定编码配置的方式进行训练。
    \item DBC6，DBC7： 训练过程使用上一章支持编码配置动态选择的DBC模型。    
\end{itemize}

这些方法在实验测试时都会使用NVTT将神经纹理导出为BC6/BC7格式进行测试。
MBD光照数据压缩实验与RGBM编码实验由于神经纹理为RGBA格式，因此
使用DBC7M模型进行实验，MBD与RGBM编码实验仅验证DBCM模型的有效性，因此这两个实验中
DBCM模型仅与使用BC7直接压缩神经纹理对比。

\paragraph{实现细节}

DBCM模型中的$N_s$与$N_r$都设为2，以兼顾计算效率与编码质量。
Gumbel配置选择器的$\tau$初始化为100，每次迭代乘0.9965521，
通过指数衰减的方式
在2000次迭代左右降为1e-4。
神经压缩实验与RBM编码实验的学习率都初始化为0.01，
MBD实验的学习率初始化为0.1。训练过程都使用Adam优化器。
对于神经压缩实验，具体配置与上一章相同，会进行10000次迭代的预训练。
对于MBD实验，进行2048次迭代的预训练。
对于RGBM编码实验，由于RGBM模型本身无需训练，
因此不进行预训练，将HDR纹理通过RGBM编码为LDR纹理作为初始化的神经纹理。

三个实验的训练过程都使用上一章中的学习率衰减策略以及停止策略，
优化完成后使用NVTT将神经纹理导出为BC6/BC7格式进行测试。

\subsection{实验结果与分析}

\subsubsection{对比实验}
\label{4:对比实验}

神经压缩实验的实验结果如图表\ref{6个材质的纹理集上进行的对比实验}所示，
相较于上一章的DBC模型，改进的DBCM模型6组实验中优化效果都好于DBC模型，
实验结果证明了DBCM模型的有效性。DBCM模型相较于DBC模型更优的原因主要有两个，
首先DBCM模型使用了退火的Gumbel配置选择器，这使得在优化初期可以对编码配置
进行一定程度的探索，同时也保证优化末期神经纹理的每个4x4块可以选到编码误差最小的配置。
其次DBCM模型在亲和度表的更新过程中加入了$\alpha=0.2$的滑动平均，
使每次优化过程更稳定，不容易在两种编码配置之间来回跳变。


\begin{table*}[htbp]
    \centering
    \caption{6个材质的纹理集上进行的对比实验}
    \label{6个材质的纹理集上进行的对比实验}        
    \resizebox{\linewidth}{!}{
    \begin{tabular}{ccccccccccccccccccc}
        \toprule
        & \multicolumn{3}{c}{Ukulele\_01} & \multicolumn{3}{c}{antique\_katana\_01} & \multicolumn{3}{c}{utility\_box\_02} & \multicolumn{3}{c}{chinese\_chandelier} & \multicolumn{3}{c}{dartboard} & \multicolumn{3}{c}{Drill\_01}\\
                  & PSNR(↑)  & SSIM(↑) & FLIP(↓)& PSNR(↑) & SSIM(↑) & FLIP(↓)& PSNR(↑) & SSIM(↑) & FLIP(↓) & PSNR(↑) & SSIM(↑)& FLIP(↓) & PSNR(↑)& SSIM(↑)& FLIP(↓)& PSNR(↑) & SSIM(↑)& FLIP(↓)\\
        \midrule
        BC6       & 22.3005  & 0.6716  & 0.2713 & 24.1197 & 0.7007  & 0.2161 & 23.8151 & 0.6977  & 0.2341  & 20.1739 & 0.604  & 0.3457 & 20.4314 & 0.5262 & 0.3073 & 22.6281 & 0.7039 & 0.2976  \\
        DBC6s     & 27.1085  & 0.7625  & 0.1705 & 27.8752 & 0.8197  & 0.1332 & 28.027  & 0.7853  & 0.1472  & 25.7551 & 0.7407 & 0.2197 & 23.7457 & 0.5955 & 0.2289 & 26.8157 & 0.8328 & 0.163  \\
        DBC6      & 27.083   & 0.7622  & 0.1707 & 27.9227 & 0.8233  & 0.1322 & 28.0362 & 0.7877  & 0.1453  & 25.8831 & 0.7453 & 0.2163 & 23.9072 & 0.5989 & 0.2245 & 26.8316 & 0.8295  & 0.1693   \\
        DBC6M     & \textbf{28.271}   & \textbf{0.7828}  & \textbf{0.1464} & \textbf{28.9463} & \textbf{0.8432}  & \textbf{0.1134} & \textbf{28.7504} & \textbf{0.8018}  & \textbf{0.1326}  & \textbf{26.8738} & \textbf{0.7626} & \textbf{0.1942} & \textbf{24.7159} & \textbf{0.6135} & \textbf{0.2078} & \textbf{28.2355} & \textbf{0.8549} & \textbf{0.1394}  \\
        \midrule
        BC7       & 23.0234  & 0.6685  & 0.2606 & 25.0525 & 0.6602  & 0.2262 & 25.0392 & 0.7163  & 0.2338  & 22.45   & 0.6358 & 0.3009 & 21.5528 & 0.5536 & 0.3052 & 22.2064 & 0.7357 & 0.2971   \\
        DBC7s     & 30.2512  & 0.8221  & 0.1087 & 30.602  & 0.8683  & 0.0919 & 30.8006 & 0.8338  & 0.1068  & 29.6132 & 0.8223 & 0.1378 & 26.6591 & 0.6551 & 0.1671 & 31.8172 & 0.9084 & 0.0986  \\
        DBC7      & 30.534   & 0.8277  & 0.1018 & 30.8209 & 0.8736  & 0.0871 & 31.154  & 0.8412  & 0.0998  & 29.9482 & 0.8297 & 0.1294 & 26.8479 & 0.6582 & 0.1643 & 33.2971 & 0.9305 & 0.0783   \\
        DBC7M     & \textbf{30.633}   & \textbf{0.8304}  & \textbf{0.0966} & \textbf{31.4472} & \textbf{0.8917}  & \textbf{0.0761} & \textbf{31.2531} & \textbf{0.844}   & \textbf{0.096}   & \textbf{30.0176} & \textbf{0.8314} & \textbf{0.1273} & \textbf{27.2132} & \textbf{0.6679} & \textbf{0.1579} & \textbf{33.6313} & \textbf{0.9326} & \textbf{0.0752}  \\
        \bottomrule
\end{tabular} }   
\end{table*}

MBD实验的结果如表\ref{MBDexp1}所示，
该表列出了5个场景中不同方法的RMSE和存储大小的具体值。
可以看出，MBD+DBCM的RMSE明显低于MBD+BC7，
降低幅度为60\%至80\%，同时占用的存储空间大小仅为MBD方法的1/4。

\begin{table*}[htbp]
    \centering
    \caption{5个烘焙的场景光照数据进行的对比实验}
    \label{MBDexp1}        
    \resizebox{\linewidth}{!}{
    \begin{tabular}{lllllllllll}
        \toprule
                    & \multicolumn{2}{l}{Cornell Box} & \multicolumn{2}{l}{Scene 0} & \multicolumn{2}{l}{Scene 1} & \multicolumn{2}{l}{Scene 2} & \multicolumn{2}{l}{Scene 3} \\
                    & RMSE       & Memory Size / KB   & RMSE     & Memory Size / KB & RMSE     & Memory Size / MB & RMSE     & Memory Size / MB & RMSE     & Memory Size / MB \\
        \midrule
    MBD + BC7       & 0.427069   & 4                  & 0.312421 & 96.25            & 0.462323 & 0.29             & 0.141934 & 0.38             & 0.415555 & 1.24             \\
    MBD + DBC7M     & 0.183869   & 4                  & 0.150063 & 96.25            & 0.292717 & 0.29             & 0.114637 & 0.38             & 0.376664 & 1.24             \\
    MBD             & 0.078967   & 16                 & 0.042723 & 385              & 0.071571 & 1.15             & 0.065373 & 1.53             & 0.198106 & 4.96             \\
    Reference       & -          & 432                & -        & 4,620            & -        & 14.13            & -        & 18.47            & -        & 59.55           \\
\bottomrule
\end{tabular} }   
\end{table*}

MBD实验中对两个场景的光照数据进行了可视化，如图\ref{fig:MBD_v3}所示
MBD+BC7的结果显示出明显的块状，
而MBD+DBC7M结果中的块状不明显。
这表明加入DBCM模型的端到端优化有效减少了BC7造成的损失。
MBD+DBC7M的结果的视觉质量与MBD的结果没有太大不同，而存储大小仅为1/4。

\begin{figure*}[htbp]
    \centering
    \includegraphics[width=1.0\linewidth]{figures/MBD_v3.pdf}
    \caption{两个场景光照数据的可视化实验}
    \label{fig:MBD_v3} 
\end{figure*}

\begin{table*}[htbp]
    % \begin{minipage}{\columnwidth}
    \caption{6个HDR图像上进行的对比实验}
    \label{tb:RGBM}
    \begin{center}
    \begin{tabular}{ccccccccccccccccccc}
    \toprule
                    & \multicolumn{3}{c}{RGBM+BC7}  & \multicolumn{3}{c}{RGBM+DBC7M}  \\ 
                    & PSNR(↑)   & SSIM(↑) & FLIP(↓)&PSNR(↑)& SSIM(↑) & FLIP(↓) \\
    \midrule
lakeside\_sunrise    & 37.4817   & 0.9666 & 0.0679 &44.1479&0.986&0.0607 \\
mirrored\_hall       & 35.3975   & 0.9148 & 0.0814 &40.5526&0.9832&0.0539 \\
stierberg\_sunrise   & 34.8078   & 0.9457 & 0.0621 &42.5311&0.9868&0.0433 \\
studio\_garden       & 35.7492   & 0.9561 & 0.1014 &44.8351&0.9918&0.0909 \\
urban\_street        & 38.7993   & 0.9748 & 0.0586 &47.4841&0.992&0.0548 \\
vignaioli\_night     & 38.3124   & 0.9814 & 0.0721 &43.5846&0.9942&0.0607 \\
    \bottomrule
    \end{tabular}
    \end{center}
    % \end{minipage}
\end{table*}

\begin{figure*}[htbp]
    \centering
    \includegraphics[width=1.0\linewidth]{figures/RGBM.pdf}
    \caption{两个HDR纹理的可视化实验}
    \label{fig:RGBM} 
\end{figure*}

RGBM编码的实验结果如表\ref{tb:RGBM}所示，
DBCM模型有效降低了BC7压缩造成的误差。
对vignaioli\_night与studio\_garden进行了可视化，结果如图\ref{fig:RGBM}所示，
RGBM+BC7方法明显造成了图块模糊，而RGBM+DBC7M方法视觉效果上接近参考图像。


\subsubsection{性能测试}

在神经压缩实验6个材质的纹理集上进行了不同方法的性能测试，
其中DBCM模型的$N_s$与$N_r$都为2。实验结果取6次实验的平均值，
如表\ref{4:6个材质的纹理集上进行的性能实验}所示。
DBCM模型由于使用了较少的编码配置数，相比上一章基于穷举法实现编码配置动态选择的DBC模型
运行速度显著提高，其中DBC7M与DBC7相比速度提升了一个数量级，
与固定编码配置的DBC7s相比耗时仅增加了一倍。
而DBC6M与DBC6相比速度同样显著提升，与固定编码配置的DBC6s相比
耗时也仅增加一倍。

\begin{table*}[htbp]
    \centering
    \caption{6个材质的纹理集上进行的性能实验}
    \label{4:6个材质的纹理集上进行的性能实验}        
    \resizebox{0.75\linewidth}{!}{
    \begin{tabular}{ccccccccccccccccccc}
        \toprule
                                    & BC6 & DBC6s & DBC6 &DBC6M& BC7  & DBC7s & DBC7 &DBC7M\\
        \midrule
        运行时间(ms)                & 33  &  80   & 1012  & 166     & 35   & 171    & 2938 &  278 \\
        优化完成的平均迭代次数      & -   &  2083  & 1975 & 1733     & -   & 3733   &  4150    &4500 \\
        优化完成的平均时间(min)     & -   &  3     & 33  &  5        & -   & 11      &  203  &21\\
        \bottomrule
\end{tabular} } 
\end{table*}

\subsubsection{参数敏感性实验}

DBCM模型中的编码配置选择器具有$N_s$与$N_r$
两个参数，为了验证改进的编码配置选择器效果，
使用不同的$N_s$与$N_r$组合在神经压缩模型中进行了实验，
实验使用 6 个材质纹理集中的chinese\_chandelier。
除$N_s$与$N_r$外，其余实验设置与之前的神经压缩实验相同。
实验结果如表\ref{tb:NsNr}所示，
其中最右边的$N_s=64$， $N_r=0$计算量相当于上一章的DBC模型。
实验结果中$N_s=N_r$时产生了最佳性能，
实现了与穷举编码配置的DBC模型相当的编码质量，
同时相对固定编码配置仅增加了少量计算开销。

\begin{table*}[htbp]
    \centering
    \caption{ $N_s$与$N_r$的参数敏感性实验结果}
    \label{tb:NsNr}
    \resizebox{\linewidth}{!}{
    \begin{tabular}{ccccccccccccccccccc}
    \toprule
    $N_s$ / $N_r$ & 0 / 2 & 1 / 1 & 2 / 0 & 0 / 4 & 2 / 2 & 4 / 0 & 0 / 8 & 4 / 4 & 8 / 0 & 64 / 0 \\
    \midrule
    PSNR (dB)     & 29.5630 & \textbf{29.9851} & 29.8575 & 29.7333 & \textbf{30.0586} & 29.9351 & 29.8061 & \textbf{30.0223} & 30.0067 & 30.0684 \\
    SSIM          & 0.8708  & \textbf{0.8737}  & 0.8701  & 0.8721  & \textbf{0.8740}  & 0.8719  & 0.8739  & \textbf{0.8741}  & 0.8740  & 0.8752  \\
    FLIP          & 0.0937  & \textbf{0.0908}  & 0.0933  & 0.0922  & \textbf{0.0903}  & 0.0922  & 0.0925  & \textbf{0.0900}  & 0.0901  & 0.0899  \\
    \bottomrule
    \end{tabular}}
\end{table*}

\section{本章小结}

本章提出了一种基于混合专家模型的分区配置选择器，
相比固定编码配置的DBCs模型，仅增加少量计算开销就能
接近穷举所有编码配置的DBC模型的效果。
引言部分提出了DBC模型穷举所有编码配置
造成大量计算开销的问题，分析了BC6与BC7的编码配置中，
分区配置占比最高的特点，从而确定从优化分区配置的角度减少计算开销。
然后分析了基于穷举法进行编码配置选择过程造成的计算量浪费问题，
并提出了一种基于混合专家模型的分区配置选择器(MoP)，
以及基于MoP改进的DBCM模型的整体框架，
之后介绍了MoP的具体实现方式。
最后在神经压缩、MBD光照数据压缩、RGBM编码
三个应用场景中对本章所提出的DBCM模型进行测试，
以证明基于MoP的DBCM模型的有效性。